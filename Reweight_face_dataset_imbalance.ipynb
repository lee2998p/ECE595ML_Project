{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfnCN_KjD4IA",
        "outputId": "65823879-3843-46e9-87e4-1c4d7450772d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-akHRu37VGD",
        "outputId": "c53b0f4d-c799-4ec7-ee21-dfbaf7346a4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Grade 4-2/ECE59500 ML/Project\n"
          ]
        }
      ],
      "source": [
        "%cd drive/My Drive/Grade 4-2/ECE59500 ML/Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGm1QZvB8SMr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PA0__j1m9Ywq"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u79PTgQoeVXC"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWh9C5SKkIL6"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RURKpIsaqTO0"
      },
      "outputs": [],
      "source": [
        "import six"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-5vgCtH2qHV"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1LxanV1eXGp"
      },
      "source": [
        "## Data Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTh-TpY8oyHL",
        "outputId": "6f7610da-7221-4b38-f69a-fa311f7cb832"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Grade 4-2/ECE59500 ML/Project/fairface\n"
          ]
        }
      ],
      "source": [
        "%cd fairface/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CG6Cu8JeZIT"
      },
      "outputs": [],
      "source": [
        "import regex as re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7DYyMAmrr81"
      },
      "outputs": [],
      "source": [
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2oIsooOirnI",
        "outputId": "9e00d50e-3a39-49b2-c2bc-f460e9f60c56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Grade 4-2/ECE59500 ML/Project/fairface/val\n"
          ]
        }
      ],
      "source": [
        "%cd val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxfvMX3Dinmw"
      },
      "outputs": [],
      "source": [
        "data = os.listdir()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CFoe2DQivoW"
      },
      "outputs": [],
      "source": [
        "data.sort(key=lambda x: int(re.sub('\\D', '', x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__2IjOdarhAR"
      },
      "outputs": [],
      "source": [
        "data_img = []\n",
        "for file in data:\n",
        "  img_read = Image.open(file)\n",
        "  img_resize = img_read.resize((28, 28))\n",
        "  img_resize = np.array(img_resize)\n",
        "  if img_resize.ndim == 2:\n",
        "    data_img.append(img_resize)\n",
        "  elif img_resize.ndim == 3:\n",
        "    data_img.append(img_resize[:, :, 0])\n",
        "data_img = np.array(data_img)\n",
        "data_img = data_img.astype(\"float32\") / 255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgpKYKLgiRMI"
      },
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVNFQS16ilN-"
      },
      "outputs": [],
      "source": [
        "with open('data_img.pkl', 'wb') as data_pickle:\n",
        "  pickle.dump(data_img, data_pickle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhtJs05fi3_t"
      },
      "outputs": [],
      "source": [
        "with open('data_img.pkl', 'rb') as data_pickle:\n",
        "  data_img = pickle.load(data_pickle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFwzJqx3cccr"
      },
      "outputs": [],
      "source": [
        "with open(\"fairface_label_val.csv\", \"r\") as csv_file:\n",
        "  reader = csv.reader(csv_file, delimiter=',')\n",
        "  data_csv = pd.DataFrame(reader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GY6n1Hgbcqc4"
      },
      "outputs": [],
      "source": [
        "data_csv = data_csv.rename(columns={0: 'file ', 1: 'age', 2:'gender', 3:'race', 4:'service_test'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1dkrwWydBRo"
      },
      "outputs": [],
      "source": [
        "data_csv.drop(index=data_csv.index[0], axis=0, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Od7XrScvdFXM"
      },
      "outputs": [],
      "source": [
        "data_csv.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNSy6SEqc-EF",
        "outputId": "6d4b10d0-367b-4df9-d869-81324b283e7d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "race\n",
              "Black              1556\n",
              "East Asian         1550\n",
              "Indian             1516\n",
              "Latino_Hispanic    1623\n",
              "Middle Eastern     1209\n",
              "Southeast Asian    1415\n",
              "White              2085\n",
              "dtype: int64"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_csv.groupby('race').size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5kvjK7NACxa"
      },
      "source": [
        "## Experiment Configuraitons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fv5Mxktr5XHF"
      },
      "outputs": [],
      "source": [
        "from collections import namedtuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_-HqfK35QjT"
      },
      "outputs": [],
      "source": [
        "Config = namedtuple('Config', [\n",
        "    'reweight', 'lr', 'num_steps', 'random', 'ratio_weighted', 'nval', 'hard_mining', 'bsize'\n",
        "])\n",
        "\n",
        "exp_repo = dict()\n",
        "\n",
        "\n",
        "def RegisterExp(name):\n",
        "    def _decorator(f):\n",
        "        exp_repo[name] = f\n",
        "        return f\n",
        "\n",
        "    return _decorator\n",
        "\n",
        "\n",
        "LR = 0.005\n",
        "NUM_STEPS = 2880\n",
        "\n",
        "\n",
        "@RegisterExp('baseline')\n",
        "def baseline_config():\n",
        "    return Config(\n",
        "        reweight=False,\n",
        "        num_steps=NUM_STEPS * 2,\n",
        "        lr=LR,\n",
        "        random=False,\n",
        "        ratio_weighted=False,\n",
        "        hard_mining=False,\n",
        "        bsize=100,\n",
        "        nval=0)\n",
        "\n",
        "\n",
        "@RegisterExp('hardmining')\n",
        "def hardmining_config():\n",
        "    return Config(\n",
        "        reweight=False,\n",
        "        num_steps=NUM_STEPS * 2,\n",
        "        lr=LR,\n",
        "        random=False,\n",
        "        ratio_weighted=False,\n",
        "        hard_mining=True,\n",
        "        bsize=500,\n",
        "        nval=0)\n",
        "\n",
        "\n",
        "@RegisterExp('ratio')\n",
        "def ratio_config():\n",
        "    return Config(\n",
        "        reweight=False,\n",
        "        num_steps=NUM_STEPS * 2,\n",
        "        lr=LR,\n",
        "        random=False,\n",
        "        ratio_weighted=True,\n",
        "        hard_mining=False,\n",
        "        bsize=100,\n",
        "        nval=0)\n",
        "\n",
        "\n",
        "@RegisterExp('random')\n",
        "def dpfish_config():\n",
        "    return Config(\n",
        "        reweight=True,\n",
        "        num_steps=NUM_STEPS * 2,\n",
        "        lr=LR,\n",
        "        random=True,\n",
        "        ratio_weighted=False,\n",
        "        hard_mining=False,\n",
        "        bsize=100,\n",
        "        nval=0)\n",
        "\n",
        "\n",
        "@RegisterExp('autodiff')\n",
        "def autodiff_config():\n",
        "    return Config(\n",
        "        reweight=True,\n",
        "        num_steps=NUM_STEPS,\n",
        "        lr=LR,\n",
        "        random=False,\n",
        "        ratio_weighted=False,\n",
        "        hard_mining=False,\n",
        "        bsize=100,\n",
        "        nval=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpQLU93TAOQQ"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qh4NUjt9-YJY"
      },
      "outputs": [],
      "source": [
        "def get_acc(logits, y):\n",
        "    prediction = tf.cast(tf.sigmoid(logits) > 0.5, tf.float32)\n",
        "    return tf.reduce_mean(tf.cast(tf.equal(prediction, y), tf.float32))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfifdgyKb3hw"
      },
      "outputs": [],
      "source": [
        "class DataSet(object):\n",
        "    def __init__(self,\n",
        "                 images,\n",
        "                 labels,\n",
        "                 seed=None):\n",
        "        #Construct a DataSet.\n",
        "        self._num_examples = images.shape[0]\n",
        "        self._images = images\n",
        "        self._labels = labels\n",
        "        self._epochs_completed = 0\n",
        "        self._index_in_epoch = 0\n",
        "        self._indices = np.arange(self._num_examples)\n",
        "\n",
        "    @property\n",
        "    def images(self):\n",
        "        return self._images.reshape(-1, 784)\n",
        "\n",
        "    @property\n",
        "    def labels(self):\n",
        "        return self._labels\n",
        "\n",
        "    @property\n",
        "    def indices(self):\n",
        "        return self._indices\n",
        "\n",
        "    @property\n",
        "    def num_examples(self):\n",
        "        return self._num_examples\n",
        "\n",
        "    @property\n",
        "    def epochs_completed(self):\n",
        "        return self._epochs_completed\n",
        "\n",
        "    def next_batch(self, batch_size, fake_data=False, shuffle=True):\n",
        "        \"\"\"Return the next `batch_size` examples (x, y, ind) from this data set.\n",
        "        `x` is image [B, 28, 28, 1]. `y` is label [B, 10], `ind` is indices [B].\n",
        "        \"\"\"\n",
        "        start = self._index_in_epoch\n",
        "        # Shuffle for the first epoch\n",
        "        if self._epochs_completed == 0 and start == 0 and shuffle:\n",
        "            perm0 = np.arange(self._num_examples)\n",
        "            np.random.shuffle(perm0)\n",
        "            self._images = self.images[perm0]\n",
        "            self._labels = self.labels[perm0]\n",
        "            self._indices = self.indices[perm0]\n",
        "        # Go to the next epoch\n",
        "        if start + batch_size > self._num_examples:\n",
        "            # Finished epoch\n",
        "            self._epochs_completed += 1\n",
        "            # Get the rest examples in this epoch\n",
        "            rest_num_examples = self._num_examples - start\n",
        "            images_rest_part = self._images[start:self._num_examples]\n",
        "            labels_rest_part = self._labels[start:self._num_examples]\n",
        "            indices_rest_part = self._indices[start:self._num_examples]\n",
        "            # Shuffle the data\n",
        "            if shuffle:\n",
        "                perm = np.arange(self._num_examples)\n",
        "                np.random.shuffle(perm)\n",
        "                self._images = self.images[perm]\n",
        "                self._labels = self.labels[perm]\n",
        "                self._indices = self.indices[perm]\n",
        "            # Start next epoch\n",
        "            start = 0\n",
        "            self._index_in_epoch = batch_size - rest_num_examples\n",
        "            end = self._index_in_epoch\n",
        "            images_new_part = self._images[start:end]\n",
        "            labels_new_part = self._labels[start:end]\n",
        "            indices_new_part = self._indices[start:end]\n",
        "            return np.concatenate(\n",
        "                (images_rest_part, images_new_part), axis=0), np.concatenate(\n",
        "                    (labels_rest_part, labels_new_part), axis=0)\n",
        "        else:\n",
        "            self._index_in_epoch += batch_size\n",
        "            end = self._index_in_epoch\n",
        "            return self._images[start:end], self._labels[start:end]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QX5gUyOuAvlV"
      },
      "outputs": [],
      "source": [
        "def get_model(inputs,\n",
        "              labels,\n",
        "              is_training=True,\n",
        "              dtype=tf.float32,\n",
        "              w_dict=None,\n",
        "              ex_wts=None,\n",
        "              reuse=None):\n",
        "    \"\"\"Builds a simple LeNet.\n",
        "    \n",
        "    :param inputs:            [Tensor]    Inputs.\n",
        "    :param labels:            [Tensor]    Labels.\n",
        "    :param is_training:       [bool]      Whether in training mode, default True.\n",
        "    :param dtype:             [dtype]     Data type, default tf.float32.\n",
        "    :param w_dict:            [dict]      Dictionary of weights, default None.\n",
        "    :param ex_wts:            [Tensor]    Example weights placeholder, default None.\n",
        "    :param reuse:             [bool]      Whether to reuse variables, default None.\n",
        "    \"\"\"\n",
        "\n",
        "    if w_dict is None:\n",
        "        w_dict = {}\n",
        "\n",
        "    def _get_var(name, shape, dtype, initializer):\n",
        "        key = tf.compat.v1.get_variable_scope().name + '/' + name\n",
        "        if key in w_dict:\n",
        "            return w_dict[key]\n",
        "        else:\n",
        "            var = tf.compat.v1.get_variable(name, shape, dtype, initializer=initializer)\n",
        "            w_dict[key] = var\n",
        "            return var\n",
        "\n",
        "    with tf.compat.v1.variable_scope('Model', reuse=reuse):\n",
        "        inputs_ = tf.cast(tf.reshape(inputs, [-1, 28, 28, 1]), dtype)\n",
        "        labels = tf.cast(labels, dtype)\n",
        "\n",
        "        w_init = tf.compat.v1.truncated_normal_initializer(stddev=0.1)\n",
        "        w1 = _get_var('w1', [5, 5, 1, 16], dtype, initializer=w_init)    # [14, 14, 16]\n",
        "        w2 = _get_var('w2', [5, 5, 16, 32], dtype, initializer=w_init)    # [7, 7, 32]\n",
        "        w3 = _get_var('w3', [5, 5, 32, 64], dtype, initializer=w_init)    # [4, 4, 64]\n",
        "        w4 = _get_var('w4', [1024, 100], dtype, initializer=w_init)\n",
        "        w5 = _get_var('w5', [100, 1], dtype, initializer=w_init)\n",
        "\n",
        "        b_init = tf.constant_initializer(0.0)\n",
        "        b1 = _get_var('b1', [16], dtype, initializer=b_init)\n",
        "        b2 = _get_var('b2', [32], dtype, initializer=b_init)\n",
        "        b3 = _get_var('b3', [64], dtype, initializer=b_init)\n",
        "        b4 = _get_var('b4', [100], dtype, initializer=b_init)\n",
        "        b5 = _get_var('b5', [1], dtype, initializer=b_init)\n",
        "\n",
        "        act = tf.nn.relu\n",
        "\n",
        "        # Conv-1\n",
        "        l0 = tf.identity(inputs_, name='l0')\n",
        "        z1 = tf.add(tf.nn.conv2d(inputs_, w1, [1, 1, 1, 1], 'SAME'), b1, name='z1')\n",
        "        l1 = act(tf.nn.max_pool(z1, [1, 3, 3, 1], [1, 2, 2, 1], 'SAME'), name='l1')\n",
        "\n",
        "        # Conv-2\n",
        "        z2 = tf.add(tf.nn.conv2d(l1, w2, [1, 1, 1, 1], 'SAME'), b2, name='z2')\n",
        "        l2 = act(tf.nn.max_pool(z2, [1, 3, 3, 1], [1, 2, 2, 1], 'SAME'), name='l2')\n",
        "\n",
        "        # Conv-3\n",
        "        z3 = tf.add(tf.nn.conv2d(l2, w3, [1, 1, 1, 1], 'SAME'), b3, name='z3')\n",
        "        l3 = act(tf.nn.max_pool(z3, [1, 3, 3, 1], [1, 2, 2, 1], 'SAME'), name='l3')\n",
        "\n",
        "        # FC-4\n",
        "        z4 = tf.add(tf.matmul(tf.reshape(l3, [-1, 1024]), w4), b4, name='z4')\n",
        "        l4 = act(z4, name='l4')\n",
        "\n",
        "        # FC-5\n",
        "        z5 = tf.add(tf.matmul(l4, w5), b5, name='z5')\n",
        "\n",
        "        logits = tf.squeeze(z5)\n",
        "        out = tf.sigmoid(logits)\n",
        "        if ex_wts is None:\n",
        "            # Average loss.\n",
        "            loss = tf.reduce_mean(\n",
        "                tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
        "        else:\n",
        "            # Weighted loss.\n",
        "            loss = tf.reduce_sum(\n",
        "                tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels) * ex_wts)\n",
        "    return w_dict, loss, logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MtWqr7FARPP"
      },
      "outputs": [],
      "source": [
        "def evaluate(sess, x_, y_, acc_, train_set, test_set):\n",
        "    # Calculate final results.\n",
        "    acc_sum = 0.0\n",
        "    acc_test_sum = 0.0\n",
        "    train_bsize = 100\n",
        "    for step in six.moves.xrange(3600 // train_bsize):\n",
        "        x, y = train_set.next_batch(train_bsize)\n",
        "        acc = sess.run(acc_, feed_dict={x_: x, y_: y})\n",
        "        acc_sum += acc\n",
        "\n",
        "    test_bsize = 100\n",
        "    for step in six.moves.xrange(360 // test_bsize):\n",
        "        x_test, y_test = test_set.next_batch(test_bsize)\n",
        "        acc = sess.run(acc_, feed_dict={x_: x_test, y_: y_test})\n",
        "        acc_test_sum += acc\n",
        "\n",
        "    train_acc = acc_sum / float(3600 // train_bsize)\n",
        "    test_acc = acc_test_sum / float(360 // test_bsize)\n",
        "    return train_acc, test_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sN3qwxPHe8O0"
      },
      "source": [
        "# Binary Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNCzBSr3CGA6"
      },
      "source": [
        "# Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxdFQk7RCTF-"
      },
      "source": [
        "## Baseline Results - Balanced Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUopR4ImAL7M"
      },
      "outputs": [],
      "source": [
        "def get_balance_dataset(data_img, seed=0):\n",
        "    rnd = np.random.RandomState(seed)\n",
        "\n",
        "    pos_ratio=0.5\n",
        "    ntrain=3600\n",
        "    nval=exp_repo['baseline']().nval\n",
        "    ntest=360\n",
        "    seed=0\n",
        "\n",
        "    ratio = 1 - pos_ratio\n",
        "    ratio_test = 0.5\n",
        "\n",
        "    class_poc = data_img[data_csv.index[data_csv['race'] != \"White\"], :, :]\n",
        "    class_caucasian = data_img[data_csv.index[data_csv['race'] == \"White\"], :, :]\n",
        "\n",
        "    class_poc_train = class_poc[0:1800, :, :]\n",
        "    class_poc_test = class_poc[1800:1800+180, :, :]\n",
        "    class_caucasian_train = class_caucasian[0:1800, :, :]\n",
        "    class_caucasian_test = class_caucasian[1800:1800+180, :, :]\n",
        "\n",
        "    x_train = np.concatenate((class_poc_train, class_caucasian_train), axis=0)\n",
        "    y_train = np.concatenate((np.zeros(1800), np.ones(1800)), axis=0)\n",
        "    x_test = np.concatenate((class_poc_test, class_caucasian_test), axis=0)\n",
        "    y_test = np.concatenate((np.zeros(180), np.ones(180)), axis=0)\n",
        "\n",
        "    train_set = DataSet(x_train * 255.0, y_train)\n",
        "    train_poc_set = DataSet(class_poc_train* 255.0, np.zeros(1800))\n",
        "    train_caucasian_set = DataSet(class_caucasian_train * 255.0, np.ones(1800))\n",
        "    test_set = DataSet(x_test * 255.0, y_test)\n",
        "\n",
        "    return train_set, test_set, train_caucasian_set, train_poc_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWTyDCrhAUll"
      },
      "outputs": [],
      "source": [
        "def run(dataset, exp_name, seed):\n",
        "    folder = os.path.join('Binary classification: Balanced Data')\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "\n",
        "    with tf.Graph().as_default(), tf.compat.v1.Session() as sess:\n",
        "        config = exp_repo[exp_name]()\n",
        "        bsize = config.bsize\n",
        "        train_set, test_set, train_pos_set, train_neg_set = get_balance_dataset(\n",
        "            dataset, seed=seed)\n",
        "        # if config.nval == 0:\n",
        "        #     val_set = BalancedDataSet(train_pos_set, train_neg_set)\n",
        "        x_ = tf.compat.v1.placeholder(tf.float32, [None, 784], name='x')\n",
        "        y_ = tf.compat.v1.placeholder(tf.float32, [None], name='y')\n",
        "        ex_wts_ = tf.compat.v1.placeholder(tf.float32, [None], name='ex_wts')\n",
        "        lr_ = tf.compat.v1.placeholder(tf.float32, [], name='lr')\n",
        "\n",
        "        # Build training model.\n",
        "        with tf.name_scope('Train'):\n",
        "            _, loss_c, logits_c = get_model(\n",
        "                x_, y_, is_training=True, dtype=tf.float32, w_dict=None, ex_wts=ex_wts_, reuse=None)\n",
        "            train_op = tf.compat.v1.train.MomentumOptimizer(config.lr, 0.9).minimize(loss_c)\n",
        "\n",
        "        print(loss_c)\n",
        "\n",
        "        # Build evaluation model.\n",
        "        with tf.name_scope('Val'):\n",
        "            _, loss_eval, logits_eval = get_model(\n",
        "                x_,\n",
        "                y_,\n",
        "                is_training=False,\n",
        "                dtype=tf.float32,\n",
        "                w_dict=None,\n",
        "                ex_wts=ex_wts_,\n",
        "                reuse=True)\n",
        "            acc_ = get_acc(logits_eval, y_)\n",
        "\n",
        "        print(loss_eval)\n",
        "\n",
        "        ex_weights_ = tf.ones([bsize], dtype=tf.float32) / float(bsize)\n",
        "        tf.summary.create_file_writer('log_dir')\n",
        "\n",
        "\n",
        "        lr = config.lr\n",
        "        num_steps = config.num_steps\n",
        "\n",
        "        acc_sum = 0.0\n",
        "        acc_test_sum = 0.0\n",
        "        loss_sum = 0.0\n",
        "        count = 0\n",
        "        sess.run(tf.compat.v1.global_variables_initializer())\n",
        "        for step in six.moves.xrange(num_steps):\n",
        "            x, y = train_set.next_batch(bsize)\n",
        "\n",
        "            # Use 50% learning rate for the second half of training.\n",
        "            if step > num_steps // 2:\n",
        "                lr = config.lr / 2.0\n",
        "            else:\n",
        "                lr = config.lr\n",
        "\n",
        "            ex_weights = sess.run(\n",
        "                ex_weights_, feed_dict={x_: x,\n",
        "                                        y_: y})\n",
        "            loss, acc, _ = sess.run(\n",
        "                [loss_c, acc_, train_op],\n",
        "                feed_dict={\n",
        "                    x_: x,\n",
        "                    y_: y,\n",
        "                    ex_wts_: ex_weights,\n",
        "                    lr_: lr\n",
        "                })\n",
        "            if (step + 1) % 72 == 0:\n",
        "                train_acc, test_acc = evaluate(sess, x_, y_, acc_, train_set, test_set)\n",
        "                print(f'Step {step+1} Loss {loss} Train acc {train_acc} Test acc {test_acc}')\n",
        "                tf.summary.create_file_writer('log_dir')\n",
        "                acc_sum = 0.0\n",
        "                loss_sum = 0.0\n",
        "                acc_test_sum = 0.0\n",
        "                count = 0\n",
        "\n",
        "        # Final evaluation.\n",
        "        train_acc, test_acc = evaluate(sess, x_, y_, acc_, train_set, test_set)\n",
        "        print('Final', 'Train acc', train_acc, 'Test acc', test_acc)\n",
        "    return train_acc, test_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lFGRSLh51l1"
      },
      "outputs": [],
      "source": [
        "def run_many(dataset, nrun):\n",
        "    train_acc_list = []\n",
        "    test_acc_list = []\n",
        "    for trial in range(nrun):\n",
        "        train_acc, test_acc = run(\n",
        "            dataset, 'baseline', (trial * 123456789) % 100000)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "\n",
        "    train_acc_list = np.array(train_acc_list)\n",
        "    test_acc_list = np.array(test_acc_list)\n",
        "    print('baseline', 'Train acc {:.3f}% ({:.3f}%)'.format(train_acc_list.mean() * 100.0,\n",
        "                                                         train_acc_list.std() * 100.0))\n",
        "    print('baseline', 'Test acc {:.3f}% ({:.3f}%)'.format(test_acc_list.mean() * 100.0,\n",
        "                                                        test_acc_list.std() * 100.0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZvaTTO_p67eS",
        "outputId": "d1d6897b-f8f2-4fd7-ca51-82ed16a98b11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensor(\"Train/Model/Sum:0\", shape=(), dtype=float32)\n",
            "Tensor(\"Val/Model/Sum:0\", shape=(), dtype=float32)\n",
            "Step 72 Loss 0.6937279105186462 Train acc 0.4999999933772617 Test acc 0.5\n",
            "Step 144 Loss 0.6931649446487427 Train acc 0.4999999917215771 Test acc 0.4933333396911621\n",
            "Step 216 Loss 0.6936889886856079 Train acc 0.4999999991721577 Test acc 0.48999998966852826\n",
            "Step 288 Loss 0.6933047771453857 Train acc 0.4999999933772617 Test acc 0.4766666690508525\n",
            "Step 360 Loss 0.6928186416625977 Train acc 0.499999994205104 Test acc 0.49333332975705463\n",
            "Step 432 Loss 0.6931508779525757 Train acc 0.4999999966886308 Test acc 0.5000000099341074\n",
            "Step 504 Loss 0.6933048963546753 Train acc 0.4999999966886308 Test acc 0.5\n",
            "Step 576 Loss 0.693379819393158 Train acc 0.4999999966886308 Test acc 0.5300000011920929\n",
            "Step 648 Loss 0.6942481994628906 Train acc 0.4999999983443154 Test acc 0.47999998927116394\n",
            "Step 720 Loss 0.6931688189506531 Train acc 0.4999999991721577 Test acc 0.5166666706403097\n",
            "Step 792 Loss 0.6924071311950684 Train acc 0.4999999983443154 Test acc 0.4966666599114736\n",
            "Step 864 Loss 0.6941707730293274 Train acc 0.499999994205104 Test acc 0.49666664997736615\n",
            "Step 936 Loss 0.69312983751297 Train acc 0.4999999975164731 Test acc 0.4966666599114736\n",
            "Step 1008 Loss 0.6930317878723145 Train acc 0.4999999975164731 Test acc 0.46666666865348816\n",
            "Step 1080 Loss 0.6933887600898743 Train acc 0.4999999933772617 Test acc 0.5166666706403097\n",
            "Step 1152 Loss 0.6934240460395813 Train acc 0.4999999966886308 Test acc 0.5133333404858907\n",
            "Step 1224 Loss 0.6929931044578552 Train acc 0.4999999975164731 Test acc 0.46333332856496173\n",
            "Step 1296 Loss 0.6936835050582886 Train acc 0.4999999966886308 Test acc 0.5099999904632568\n",
            "Step 1368 Loss 0.6930685639381409 Train acc 0.4999999958607886 Test acc 0.4966666599114736\n",
            "Step 1440 Loss 0.6936269402503967 Train acc 0.4999999966886308 Test acc 0.5166666706403097\n",
            "Step 1512 Loss 0.6926634311676025 Train acc 0.499999994205104 Test acc 0.4766666690508525\n",
            "Step 1584 Loss 0.6929302215576172 Train acc 0.4999999975164731 Test acc 0.5100000003973643\n",
            "Step 1656 Loss 0.6930049061775208 Train acc 0.4999999933772617 Test acc 0.47999998927116394\n",
            "Step 1728 Loss 0.692695677280426 Train acc 0.4999999975164731 Test acc 0.47999998927116394\n",
            "Step 1800 Loss 0.6951767802238464 Train acc 0.499999994205104 Test acc 0.5199999908606211\n",
            "Step 1872 Loss 0.693164050579071 Train acc 0.5000000008278422 Test acc 0.4899999996026357\n",
            "Step 1944 Loss 0.6942337155342102 Train acc 0.4999999933772617 Test acc 0.4999999900658925\n",
            "Step 2016 Loss 0.6930017471313477 Train acc 0.4999999958607886 Test acc 0.48999998966852826\n",
            "Step 2088 Loss 0.693805992603302 Train acc 0.4999999983443154 Test acc 0.49333332975705463\n",
            "Step 2160 Loss 0.6931645274162292 Train acc 0.4999999983443154 Test acc 0.4933333396911621\n",
            "Step 2232 Loss 0.6929644346237183 Train acc 0.4999999983443154 Test acc 0.503333330154419\n",
            "Step 2304 Loss 0.6928080916404724 Train acc 0.4999999917215771 Test acc 0.4666666587193807\n",
            "Step 2376 Loss 0.694263219833374 Train acc 0.4999999950329463 Test acc 0.549999992052714\n",
            "Step 2448 Loss 0.692836344242096 Train acc 0.4999999983443154 Test acc 0.4766666690508525\n",
            "Step 2520 Loss 0.6935796737670898 Train acc 0.4999999950329463 Test acc 0.5200000007947286\n",
            "Step 2592 Loss 0.6932679414749146 Train acc 0.499999994205104 Test acc 0.5099999904632568\n",
            "Step 2664 Loss 0.6938138604164124 Train acc 0.4999999933772617 Test acc 0.49333332975705463\n",
            "Step 2736 Loss 0.6935765743255615 Train acc 0.4999999950329463 Test acc 0.5200000107288361\n",
            "Step 2808 Loss 0.693044900894165 Train acc 0.5000000008278422 Test acc 0.5033333400885264\n",
            "Step 2880 Loss 0.6934191584587097 Train acc 0.4999999975164731 Test acc 0.5200000107288361\n",
            "Step 2952 Loss 0.6926098465919495 Train acc 0.4999999975164731 Test acc 0.4933333396911621\n",
            "Step 3024 Loss 0.6934770345687866 Train acc 0.499999994205104 Test acc 0.5099999904632568\n",
            "Step 3096 Loss 0.6932201981544495 Train acc 0.4999999975164731 Test acc 0.5000000099341074\n",
            "Step 3168 Loss 0.6935712099075317 Train acc 0.4999999950329463 Test acc 0.503333330154419\n",
            "Step 3240 Loss 0.6945875287055969 Train acc 0.4999999983443154 Test acc 0.4999999900658925\n",
            "Step 3312 Loss 0.6929681301116943 Train acc 0.4999999950329463 Test acc 0.5066666801770529\n",
            "Step 3384 Loss 0.692592978477478 Train acc 0.4999999950329463 Test acc 0.5166666507720947\n",
            "Step 3456 Loss 0.6937875747680664 Train acc 0.499999994205104 Test acc 0.48666666944821674\n",
            "Step 3528 Loss 0.6931853890419006 Train acc 0.499999994205104 Test acc 0.5066666603088379\n",
            "Step 3600 Loss 0.6941359639167786 Train acc 0.4999999950329463 Test acc 0.5133333305517832\n",
            "Step 3672 Loss 0.6943237781524658 Train acc 0.5000000008278422 Test acc 0.4799999992052714\n",
            "Step 3744 Loss 0.6933532953262329 Train acc 0.4999999983443154 Test acc 0.5333333412806193\n",
            "Step 3816 Loss 0.6936224699020386 Train acc 0.4999999925494194 Test acc 0.526666651169459\n",
            "Step 3888 Loss 0.6932687163352966 Train acc 0.499999994205104 Test acc 0.49333332975705463\n",
            "Step 3960 Loss 0.6932179927825928 Train acc 0.4999999983443154 Test acc 0.5400000015894572\n",
            "Step 4032 Loss 0.6930565237998962 Train acc 0.4999999966886308 Test acc 0.5466666519641876\n",
            "Step 4104 Loss 0.6918746829032898 Train acc 0.4999999983443154 Test acc 0.49333332975705463\n",
            "Step 4176 Loss 0.6932117938995361 Train acc 0.4999999966886308 Test acc 0.49333332975705463\n",
            "Step 4248 Loss 0.6932767629623413 Train acc 0.4999999991721577 Test acc 0.5133333305517832\n",
            "Step 4320 Loss 0.6923151612281799 Train acc 0.4999999983443154 Test acc 0.5066666603088379\n",
            "Step 4392 Loss 0.6934243440628052 Train acc 0.4999999925494194 Test acc 0.4866666595141093\n",
            "Step 4464 Loss 0.6950910687446594 Train acc 0.4999999983443154 Test acc 0.4866666595141093\n",
            "Step 4536 Loss 0.6937334537506104 Train acc 0.4999999975164731 Test acc 0.4799999992052714\n",
            "Step 4608 Loss 0.6934996843338013 Train acc 0.4999999991721577 Test acc 0.4899999996026357\n",
            "Step 4680 Loss 0.6933465600013733 Train acc 0.4999999925494194 Test acc 0.48999998966852826\n",
            "Step 4752 Loss 0.6923394203186035 Train acc 0.4999999966886308 Test acc 0.4999999900658925\n",
            "Step 4824 Loss 0.6931857466697693 Train acc 0.4999999917215771 Test acc 0.5066666603088379\n",
            "Step 4896 Loss 0.6930289268493652 Train acc 0.4999999958607886 Test acc 0.4999999900658925\n",
            "Step 4968 Loss 0.6934930086135864 Train acc 0.499999994205104 Test acc 0.4866666595141093\n",
            "Step 5040 Loss 0.6932789087295532 Train acc 0.4999999975164731 Test acc 0.5\n",
            "Step 5112 Loss 0.6930927634239197 Train acc 0.4999999950329463 Test acc 0.5100000103314718\n",
            "Step 5184 Loss 0.6931910514831543 Train acc 0.4999999958607886 Test acc 0.5166666607062022\n",
            "Step 5256 Loss 0.6928943395614624 Train acc 0.4999999966886308 Test acc 0.5033333400885264\n",
            "Step 5328 Loss 0.6927500367164612 Train acc 0.4999999991721577 Test acc 0.5266666611035665\n",
            "Step 5400 Loss 0.6930310726165771 Train acc 0.5 Test acc 0.5633333325386047\n",
            "Step 5472 Loss 0.6933932900428772 Train acc 0.4999999975164731 Test acc 0.5366666615009308\n",
            "Step 5544 Loss 0.6946862936019897 Train acc 0.4999999983443154 Test acc 0.49333332975705463\n",
            "Step 5616 Loss 0.6933740973472595 Train acc 0.4999999966886308 Test acc 0.503333330154419\n",
            "Step 5688 Loss 0.6931580305099487 Train acc 0.4999999950329463 Test acc 0.49666666984558105\n",
            "Step 5760 Loss 0.6930004358291626 Train acc 0.4999999991721577 Test acc 0.5100000003973643\n",
            "Final Train acc 0.4999999950329463 Test acc 0.5066666603088379\n",
            "Tensor(\"Train/Model/Sum:0\", shape=(), dtype=float32)\n",
            "Tensor(\"Val/Model/Sum:0\", shape=(), dtype=float32)\n",
            "Step 72 Loss 0.8322151899337769 Train acc 0.4999999983443154 Test acc 0.5166666706403097\n",
            "Step 144 Loss 1.254549503326416 Train acc 0.4999999958607886 Test acc 0.47333332896232605\n",
            "Step 216 Loss 0.6862313747406006 Train acc 0.4999999950329463 Test acc 0.4833333392937978\n",
            "Step 288 Loss 0.6897812485694885 Train acc 0.5000000008278422 Test acc 0.4933333198229472\n",
            "Step 360 Loss 0.6951751112937927 Train acc 0.4999999983443154 Test acc 0.4666666587193807\n",
            "Step 432 Loss 0.7134673595428467 Train acc 0.4999999950329463 Test acc 0.4799999992052714\n",
            "Step 504 Loss 0.7022143006324768 Train acc 0.4999999958607886 Test acc 0.49666667977968854\n",
            "Step 576 Loss 0.6927610039710999 Train acc 0.4999999975164731 Test acc 0.5066666603088379\n",
            "Step 648 Loss 0.7056839466094971 Train acc 0.4999999983443154 Test acc 0.5166666507720947\n",
            "Step 720 Loss 0.6964946985244751 Train acc 0.499999994205104 Test acc 0.4899999996026357\n",
            "Step 792 Loss 0.6914858818054199 Train acc 0.4999999975164731 Test acc 0.4833333392937978\n",
            "Step 864 Loss 0.7176632285118103 Train acc 0.499999994205104 Test acc 0.5066666702429453\n",
            "Step 936 Loss 0.7380815744400024 Train acc 0.4999999991721577 Test acc 0.48666666944821674\n",
            "Step 1008 Loss 0.7100363373756409 Train acc 0.4999999958607886 Test acc 0.4699999888737996\n",
            "Step 1080 Loss 0.6933706402778625 Train acc 0.4999999975164731 Test acc 0.5366666714350382\n",
            "Step 1152 Loss 0.693464994430542 Train acc 0.4999999966886308 Test acc 0.5433333317438761\n",
            "Step 1224 Loss 0.7136338949203491 Train acc 0.5 Test acc 0.476666659116745\n",
            "Step 1296 Loss 0.6929751038551331 Train acc 0.4999999975164731 Test acc 0.4999999801317851\n",
            "Step 1368 Loss 0.6909499764442444 Train acc 0.4999999991721577 Test acc 0.4899999996026357\n",
            "Step 1440 Loss 0.6913461685180664 Train acc 0.4999999958607886 Test acc 0.5166666607062022\n",
            "Step 1512 Loss 0.6954238414764404 Train acc 0.4999999950329463 Test acc 0.4700000087420146\n",
            "Step 1584 Loss 0.6913473010063171 Train acc 0.4999999950329463 Test acc 0.49333332975705463\n",
            "Step 1656 Loss 0.6926758289337158 Train acc 0.4999999991721577 Test acc 0.503333330154419\n",
            "Step 1728 Loss 0.6858010292053223 Train acc 0.4999999917215771 Test acc 0.4933333396911621\n",
            "Step 1800 Loss 0.6941531300544739 Train acc 0.4999999983443154 Test acc 0.4766666690508525\n",
            "Step 1872 Loss 0.6928781867027283 Train acc 0.5000000008278422 Test acc 0.4833333392937978\n",
            "Step 1944 Loss 0.6880286931991577 Train acc 0.4999999950329463 Test acc 0.49666664997736615\n",
            "Step 2016 Loss 0.6958603858947754 Train acc 0.4999999958607886 Test acc 0.5266666611035665\n",
            "Step 2088 Loss 0.6924877166748047 Train acc 0.499999994205104 Test acc 0.5200000007947286\n",
            "Step 2160 Loss 0.687065839767456 Train acc 0.4999999966886308 Test acc 0.4966666599114736\n",
            "Step 2232 Loss 0.6925328373908997 Train acc 0.4999999950329463 Test acc 0.48999998966852826\n",
            "Step 2304 Loss 0.6929540634155273 Train acc 0.4999999975164731 Test acc 0.52333332101504\n",
            "Step 2376 Loss 0.6943337321281433 Train acc 0.4999999975164731 Test acc 0.4766666690508525\n",
            "Step 2448 Loss 0.6950857043266296 Train acc 0.4999999958607886 Test acc 0.5133333305517832\n",
            "Step 2520 Loss 0.6907899975776672 Train acc 0.4999999983443154 Test acc 0.48666666944821674\n",
            "Step 2592 Loss 0.6990759372711182 Train acc 0.5000000049670538 Test acc 0.5099999805291494\n",
            "Step 2664 Loss 0.6838089227676392 Train acc 0.4999999975164731 Test acc 0.49666666984558105\n",
            "Step 2736 Loss 0.6923701763153076 Train acc 0.4999999991721577 Test acc 0.5066666603088379\n",
            "Step 2808 Loss 0.6973978281021118 Train acc 0.4999999933772617 Test acc 0.5166666607062022\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-e96177eaa4b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-beaa8325ba0f>\u001b[0m in \u001b[0;36mrun_many\u001b[0;34m(dataset, nrun)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtrial\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         train_acc, test_acc = run(\n\u001b[0;32m----> 6\u001b[0;31m             dataset, 'baseline', (trial * 123456789) % 100000)\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mtrain_acc_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtest_acc_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-5613f4287066>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(dataset, exp_name, seed)\u001b[0m\n\u001b[1;32m     68\u001b[0m                     \u001b[0my_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                     \u001b[0mex_wts_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mex_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                     \u001b[0mlr_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m                 })\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m72\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 968\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    969\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1191\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1192\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1369\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1371\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1372\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1375\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1377\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1378\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1359\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1361\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1453\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1454\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1455\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1457\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "run_many(data_img, nrun=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxOh-CHrCcjw"
      },
      "source": [
        "# Reweighted Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHvedymDJdv7"
      },
      "outputs": [],
      "source": [
        "def get_imbalance_dataset(data_img, exp_name, seed, pos_ratio):\n",
        "    ntrain=3600\n",
        "    nval=exp_repo[exp_name]().nval\n",
        "    ntest=360\n",
        "    seed=0\n",
        "\n",
        "    ratio = 1 - pos_ratio\n",
        "    ratio_test = 0.5\n",
        "    rnd = np.random.RandomState(seed)\n",
        "\n",
        "    class_poc = data_img[data_csv.index[data_csv['race'] != \"White\"], :, :]\n",
        "    class_caucasian = data_img[data_csv.index[data_csv['race'] == \"White\"], :, :]\n",
        "\n",
        "    x_train_poc, x_test_poc = train_test_split(class_poc, test_size=0.1, random_state=25)\n",
        "    x_train_caucasian, x_test_caucasian = train_test_split(class_caucasian, test_size=0.1, random_state=25)\n",
        "\n",
        "    # First shuffle, negative.\n",
        "    idx = np.arange(x_train_poc.shape[0])\n",
        "    rnd.shuffle(idx)\n",
        "    x_train_poc = x_train_poc[idx]\n",
        "\n",
        "    nval_small_neg = int(np.floor(nval * ratio_test))\n",
        "    ntrain_small_neg = int(np.floor(ntrain * ratio)) - nval_small_neg\n",
        "\n",
        "    x_val_poc = x_train_poc[:nval_small_neg]    # 450 4 in validation.\n",
        "    x_train_poc = x_train_poc[nval_small_neg:nval_small_neg + ntrain_small_neg]    # 500 4 in training.\n",
        "\n",
        "    idx = np.arange(x_test_poc.shape[0])\n",
        "    rnd.shuffle(idx)\n",
        "    x_test_poc = x_test_poc[:int(np.floor(ntest * ratio_test))]    # 450 4 in testing.\n",
        "\n",
        "    # First shuffle, positive.\n",
        "    idx = np.arange(x_train_caucasian.shape[0])\n",
        "    rnd.shuffle(idx)\n",
        "    x_train_caucasian = x_train_caucasian[idx]\n",
        "\n",
        "    nvalsmall_pos = int(np.floor(nval * (1 - ratio_test)))\n",
        "    ntrainsmall_pos = int(np.floor(ntrain * (1 - ratio))) - nvalsmall_pos\n",
        "\n",
        "    x_val_caucasian = x_train_caucasian[:nvalsmall_pos]    # 50 9 in validation.\n",
        "    x_train_caucasian = x_train_caucasian[nvalsmall_pos:nvalsmall_pos + ntrainsmall_pos]    # 4500 9 in training.\n",
        "\n",
        "    idx = np.arange(x_test_caucasian.shape[0])\n",
        "    rnd.shuffle(idx)\n",
        "    x_test_caucasian = x_test_caucasian[idx]\n",
        "    x_test_caucasian = x_test_caucasian[:int(np.floor(ntest * (1 - ratio_test)))]    # 500 9 in testing.\n",
        "    \n",
        "    y_train_subset = np.concatenate([np.zeros([x_train_poc.shape[0]]), np.ones([x_train_caucasian.shape[0]])])\n",
        "    y_val_subset = np.concatenate([np.zeros([x_val_poc.shape[0]]), np.ones([x_val_caucasian.shape[0]])])\n",
        "    y_test_subset = np.concatenate([np.zeros([x_test_poc.shape[0]]), np.ones([x_test_caucasian.shape[0]])])\n",
        "\n",
        "    y_train_pos_subset = np.ones([x_train_caucasian.shape[0]])\n",
        "    y_train_neg_subset = np.zeros([x_train_poc.shape[0]])\n",
        "\n",
        "    x_train_subset = np.concatenate([x_train_poc, x_train_caucasian], axis=0).reshape([-1, 28, 28, 1])\n",
        "    x_val_subset = np.concatenate([x_val_poc, x_val_caucasian], axis=0).reshape([-1, 28, 28, 1])\n",
        "    x_test_subset = np.concatenate([x_test_poc, x_test_caucasian], axis=0).reshape([-1, 28, 28, 1])\n",
        "\n",
        "    x_train_pos_subset = x_train_caucasian.reshape([-1, 28, 28, 1])\n",
        "    x_train_neg_subset = x_train_poc.reshape([-1, 28, 28, 1])\n",
        "\n",
        "    # Final shuffle.\n",
        "    idx = np.arange(x_train_subset.shape[0])\n",
        "    rnd.shuffle(idx)\n",
        "    x_train_subset = x_train_subset[idx]\n",
        "    y_train_subset = y_train_subset[idx]\n",
        "\n",
        "    idx = np.arange(x_val_subset.shape[0])\n",
        "    rnd.shuffle(idx)\n",
        "    x_val_subset = x_val_subset[idx]\n",
        "    y_val_subset = y_val_subset[idx]\n",
        "\n",
        "    idx = np.arange(x_test_subset.shape[0])\n",
        "    rnd.shuffle(idx)\n",
        "    x_test_subset = x_test_subset[idx]\n",
        "    y_test_subset = y_test_subset[idx]\n",
        "\n",
        "    train_set = DataSet(x_train_subset * 255.0, y_train_subset)\n",
        "    train_pos_set = DataSet(x_train_pos_subset * 255.0, y_train_pos_subset)\n",
        "    train_neg_set = DataSet(x_train_neg_subset * 255.0, y_train_neg_subset)\n",
        "    val_set = DataSet(x_val_subset * 255.0, y_val_subset)\n",
        "    test_set = DataSet(x_test_subset * 255.0, y_test_subset)\n",
        "\n",
        "    return train_set, val_set, test_set, train_pos_set, train_neg_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpGizMozGmYc"
      },
      "outputs": [],
      "source": [
        "def run_imbalance(dataset, exp_name, seed, pos_ratio):\n",
        "    folder = os.path.join('Binary classification: Imbalanced Data')\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "\n",
        "    with tf.Graph().as_default(), tf.compat.v1.Session() as sess:\n",
        "        config = exp_repo[exp_name]()\n",
        "        bsize = config.bsize\n",
        "        nval = config.nval\n",
        "        train_set, val_set, test_set, train_pos_set, train_neg_set = get_imbalance_dataset(\n",
        "            dataset, exp_name, seed, pos_ratio)\n",
        "        x_ = tf.compat.v1.placeholder(tf.float32, [None, 784], name='x')\n",
        "        y_ = tf.compat.v1.placeholder(tf.float32, [None], name='y')\n",
        "        x_val_ = tf.compat.v1.placeholder(tf.float32, [None, 784], name='x_val')\n",
        "        y_val_ = tf.compat.v1.placeholder(tf.float32, [None], name='y_val')\n",
        "        ex_wts_ = tf.compat.v1.placeholder(tf.float32, [None], name='ex_wts')\n",
        "        lr_ = tf.compat.v1.placeholder(tf.float32, [], name='lr')\n",
        "\n",
        "        # Build training model.\n",
        "        with tf.name_scope('Train'):\n",
        "            _, loss_c, logits_c = get_model(\n",
        "                x_, y_, is_training=True, dtype=tf.float32, w_dict=None, ex_wts=ex_wts_, reuse=None)\n",
        "            train_op = tf.compat.v1.train.MomentumOptimizer(config.lr, 0.9).minimize(loss_c)\n",
        "\n",
        "        # Build evaluation model.\n",
        "        with tf.name_scope('Val'):\n",
        "            _, loss_eval, logits_eval = get_model(\n",
        "                x_,\n",
        "                y_,\n",
        "                is_training=False,\n",
        "                dtype=tf.float32,\n",
        "                w_dict=None,\n",
        "                ex_wts=ex_wts_,\n",
        "                reuse=True)\n",
        "            acc_ = get_acc(logits_eval, y_)\n",
        "\n",
        "        ex_weights_ = tf.ones([bsize], dtype=tf.float32) / float(bsize)\n",
        "        tf.summary.create_file_writer('log_dir')\n",
        "\n",
        "        if config.reweight:\n",
        "            if config.random:\n",
        "                ex_weights_ = reweight_random(bsize)\n",
        "            else:\n",
        "                ex_weights_ = reweight_autodiff(\n",
        "                    x_,\n",
        "                    y_,\n",
        "                    x_val_,\n",
        "                    y_val_,\n",
        "                    bsize,\n",
        "                    min(bsize, nval),\n",
        "                    eps=0.0,\n",
        "                    gate_gradients=1)\n",
        "        else:\n",
        "            if config.hard_mining:\n",
        "                ex_weights_ = reweight_hard_mining(x_, y_, positive=True)\n",
        "            else:\n",
        "                if config.ratio_weighted:\n",
        "                    # Weighted by the ratio of each class.\n",
        "                    ex_weights_ = pos_ratio * (1 - y_) + (1 - pos_ratio) * (y_)\n",
        "                else:\n",
        "                    # Weighted by uniform.\n",
        "                    ex_weights_ = tf.ones([bsize], dtype=tf.float32) / float(bsize)\n",
        "\n",
        "        lr = config.lr\n",
        "        num_steps = config.num_steps\n",
        "\n",
        "        acc_sum = 0.0\n",
        "        acc_test_sum = 0.0\n",
        "        loss_sum = 0.0\n",
        "        count = 0\n",
        "        sess.run(tf.compat.v1.global_variables_initializer())\n",
        "        for step in six.moves.xrange(num_steps):\n",
        "            x, y = train_set.next_batch(bsize)\n",
        "            x_val, y_val = val_set.next_batch(min(bsize, nval))\n",
        "\n",
        "            # Use 50% learning rate for the second half of training.\n",
        "            if step > num_steps // 2:\n",
        "                lr = config.lr / 2.0\n",
        "            else:\n",
        "                lr = config.lr\n",
        "\n",
        "            ex_weights = sess.run(\n",
        "                ex_weights_, feed_dict={x_: x,\n",
        "                                        y_: y,\n",
        "                                        x_val_: x_val,\n",
        "                                        y_val_: y_val})\n",
        "            loss, acc, _ = sess.run(\n",
        "                [loss_c, acc_, train_op],\n",
        "                feed_dict={\n",
        "                    x_: x,\n",
        "                    y_: y,\n",
        "                    x_val_: x_val,\n",
        "                    y_val_: y_val,\n",
        "                    ex_wts_: ex_weights,\n",
        "                    lr_: lr\n",
        "                })\n",
        "            if (step + 1) % 72 == 0:\n",
        "                train_acc, test_acc = evaluate(sess, x_, y_, acc_, train_set, test_set)\n",
        "                print('Step', step + 1, 'Loss', loss, 'Train acc', train_acc, 'Test acc',\n",
        "                          test_acc)\n",
        "                acc_sum = 0.0\n",
        "                loss_sum = 0.0\n",
        "                acc_test_sum = 0.0\n",
        "                count = 0\n",
        "\n",
        "        # Final evaluation.\n",
        "        train_acc, test_acc = evaluate(sess, x_, y_, acc_, train_set, test_set)\n",
        "        print('Final', 'Train acc', train_acc, 'Test acc', test_acc)\n",
        "    return train_acc, test_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81gGxQ4yHQId"
      },
      "outputs": [],
      "source": [
        "def run_many_imbalance(dataset, exp_name, nrun):\n",
        "    results = []\n",
        "    for pos_ratio in np.arange(0.6, 1, 0.1):\n",
        "      train_acc_list = []\n",
        "      test_acc_list = []\n",
        "      for trial in range(nrun):\n",
        "          train_acc, test_acc = run_imbalance(\n",
        "              dataset, exp_name, (trial * 123456789) % 100000, pos_ratio)\n",
        "          train_acc_list.append(train_acc)\n",
        "          test_acc_list.append(test_acc)\n",
        "\n",
        "      train_acc_list = np.array(train_acc_list)\n",
        "      test_acc_list = np.array(test_acc_list)\n",
        "      print(exp_name, pos_ratio, 'Train acc {:.3f}% ({:.3f}%)'.format(train_acc_list.mean() * 100.0,\n",
        "                                                          train_acc_list.std() * 100.0))\n",
        "      print(exp_name, pos_ratio, 'Test acc {:.3f}% ({:.3f}%)'.format(test_acc_list.mean() * 100.0,\n",
        "                                                          test_acc_list.std() * 100.0))\n",
        "      results.append([train_acc_list.mean() * 100.0, test_acc_list.mean() * 100.0])\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-A7D7s8H6q3",
        "outputId": "fc7ce563-655b-4682-e160-f693dccf9a7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 72 Loss nan Train acc 0.4283333321412404 Test acc 0.476666659116745\n",
            "Step 144 Loss nan Train acc 0.4324999981456333 Test acc 0.5233333309491476\n",
            "Step 216 Loss nan Train acc 0.4366666649778684 Test acc 0.5299999912579855\n",
            "Step 288 Loss nan Train acc 0.43611110912428963 Test acc 0.46666666865348816\n",
            "Step 360 Loss nan Train acc 0.430277777214845 Test acc 0.4999999900658925\n",
            "Step 432 Loss nan Train acc 0.4275000012583203 Test acc 0.5033333202203115\n",
            "Step 504 Loss nan Train acc 0.43416666405068505 Test acc 0.503333330154419\n",
            "Step 576 Loss nan Train acc 0.42944444302055573 Test acc 0.5033333202203115\n",
            "Step 648 Loss nan Train acc 0.42916666385200286 Test acc 0.48333332935969037\n",
            "Step 720 Loss nan Train acc 0.43888888508081436 Test acc 0.49666666984558105\n",
            "Step 792 Loss nan Train acc 0.43583333326710594 Test acc 0.503333330154419\n",
            "Step 864 Loss nan Train acc 0.4324999964899487 Test acc 0.5100000003973643\n",
            "Step 936 Loss nan Train acc 0.42638888706763584 Test acc 0.5133333404858907\n",
            "Step 1008 Loss nan Train acc 0.4374999975164731 Test acc 0.4933333198229472\n",
            "Step 1080 Loss nan Train acc 0.4324999948342641 Test acc 0.4933333396911621\n",
            "Step 1152 Loss nan Train acc 0.43888888839218354 Test acc 0.5233333309491476\n",
            "Step 1224 Loss nan Train acc 0.426388890379005 Test acc 0.4699999988079071\n",
            "Step 1296 Loss nan Train acc 0.4372222191757626 Test acc 0.5066666702429453\n",
            "Step 1368 Loss nan Train acc 0.43666666663355297 Test acc 0.48999998966852826\n",
            "Step 1440 Loss nan Train acc 0.43638888663715786 Test acc 0.5133333305517832\n",
            "Step 1512 Loss nan Train acc 0.43555555244286853 Test acc 0.48999998966852826\n",
            "Step 1584 Loss nan Train acc 0.43555555161502624 Test acc 0.52333332101504\n",
            "Step 1656 Loss nan Train acc 0.4374999983443154 Test acc 0.47333332896232605\n",
            "Step 1728 Loss nan Train acc 0.4238888877961371 Test acc 0.5100000103314718\n",
            "Step 1800 Loss nan Train acc 0.4355555532707108 Test acc 0.4933333198229472\n",
            "Step 1872 Loss nan Train acc 0.43027777555916047 Test acc 0.4966666599114736\n",
            "Step 1944 Loss nan Train acc 0.42694444126553005 Test acc 0.4999999900658925\n",
            "Step 2016 Loss nan Train acc 0.4352777724464734 Test acc 0.5099999904632568\n",
            "Step 2088 Loss nan Train acc 0.4344444415635533 Test acc 0.5100000003973643\n",
            "Step 2160 Loss nan Train acc 0.4352777749300003 Test acc 0.48999998966852826\n",
            "Step 2232 Loss nan Train acc 0.43388888653781676 Test acc 0.5066666603088379\n",
            "Step 2304 Loss nan Train acc 0.4299999963906076 Test acc 0.49666667977968854\n",
            "Step 2376 Loss nan Train acc 0.4322222214606073 Test acc 0.5033333400885264\n",
            "Step 2448 Loss nan Train acc 0.4338888857099745 Test acc 0.4866666595141093\n",
            "Step 2520 Loss nan Train acc 0.4286111096541087 Test acc 0.4999999900658925\n",
            "Step 2592 Loss nan Train acc 0.4330555531713698 Test acc 0.5066666702429453\n",
            "Step 2664 Loss nan Train acc 0.4349999949336052 Test acc 0.5233333309491476\n",
            "Step 2736 Loss nan Train acc 0.4308333305848969 Test acc 0.4699999988079071\n",
            "Step 2808 Loss nan Train acc 0.4286111096541087 Test acc 0.5366666515668234\n",
            "Step 2880 Loss nan Train acc 0.43166666229565936 Test acc 0.5166666706403097\n",
            "Final Train acc 0.4324999940064218 Test acc 0.476666659116745\n",
            "autodiff 0.6 Train acc 43.250% (0.000%)\n",
            "autodiff 0.6 Test acc 47.667% (0.000%)\n",
            "Step 72 Loss nan Train acc 0.35722222220566535 Test acc 0.5033333400885264\n",
            "Step 144 Loss nan Train acc 0.3636111112104522 Test acc 0.4933333198229472\n",
            "Step 216 Loss nan Train acc 0.3680555563833978 Test acc 0.4799999992052714\n",
            "Step 288 Loss nan Train acc 0.3638888895511627 Test acc 0.5433333218097687\n",
            "Step 360 Loss nan Train acc 0.3569444451067183 Test acc 0.4633333186308543\n",
            "Step 432 Loss nan Train acc 0.366111110481951 Test acc 0.5166666607062022\n",
            "Step 504 Loss nan Train acc 0.3677777796983719 Test acc 0.48999998966852826\n",
            "Step 576 Loss nan Train acc 0.36416666706403095 Test acc 0.5233333309491476\n",
            "Step 648 Loss nan Train acc 0.36111111069719 Test acc 0.5299999912579855\n",
            "Step 720 Loss nan Train acc 0.3636111128661368 Test acc 0.47333332896232605\n",
            "Step 792 Loss nan Train acc 0.363055557012558 Test acc 0.476666659116745\n",
            "Step 864 Loss nan Train acc 0.36444444457689923 Test acc 0.5066666603088379\n",
            "Step 936 Loss nan Train acc 0.3680555547277133 Test acc 0.5066666702429453\n",
            "Step 1008 Loss nan Train acc 0.3705555556548966 Test acc 0.5166666706403097\n",
            "Step 1080 Loss nan Train acc 0.36444444706042606 Test acc 0.46666666865348816\n",
            "Step 1152 Loss nan Train acc 0.3702777773141861 Test acc 0.5366666714350382\n",
            "Step 1224 Loss nan Train acc 0.35805555764171815 Test acc 0.47333332896232605\n",
            "Step 1296 Loss nan Train acc 0.3627777761883206 Test acc 0.5\n",
            "Step 1368 Loss nan Train acc 0.3633333345254262 Test acc 0.5066666603088379\n",
            "Step 1440 Loss nan Train acc 0.36444444623258376 Test acc 0.4966666599114736\n",
            "Step 1512 Loss nan Train acc 0.3630555561847157 Test acc 0.5033333202203115\n",
            "Step 1584 Loss nan Train acc 0.36277778032753205 Test acc 0.5066666603088379\n",
            "Step 1656 Loss nan Train acc 0.3674999972184499 Test acc 0.4966666599114736\n",
            "Step 1728 Loss nan Train acc 0.3663888896505038 Test acc 0.4899999996026357\n",
            "Step 1800 Loss nan Train acc 0.3638888887233204 Test acc 0.4933333198229472\n",
            "Step 1872 Loss nan Train acc 0.3588888926638497 Test acc 0.5233333309491476\n",
            "Step 1944 Loss nan Train acc 0.3591666668653488 Test acc 0.4966666599114736\n",
            "Step 2016 Loss nan Train acc 0.35999999774826896 Test acc 0.5166666706403097\n",
            "Step 2088 Loss nan Train acc 0.3697222231162919 Test acc 0.4566666583220164\n",
            "Step 2160 Loss nan Train acc 0.3655555546283722 Test acc 0.5133333206176758\n",
            "Step 2232 Loss nan Train acc 0.3655555554562145 Test acc 0.49333332975705463\n",
            "Step 2304 Loss nan Train acc 0.3666666696468989 Test acc 0.5166666507720947\n",
            "Step 2376 Loss nan Train acc 0.36444444706042606 Test acc 0.4699999988079071\n",
            "Step 2448 Loss nan Train acc 0.36222222199042636 Test acc 0.5000000099341074\n",
            "Step 2520 Loss nan Train acc 0.3683333343101872 Test acc 0.5100000003973643\n",
            "Step 2592 Loss nan Train acc 0.3630555561847157 Test acc 0.5099999904632568\n",
            "Step 2664 Loss nan Train acc 0.36611111130979324 Test acc 0.5200000107288361\n",
            "Step 2736 Loss nan Train acc 0.36249999867545235 Test acc 0.47999998927116394\n",
            "Step 2808 Loss nan Train acc 0.3608333335982429 Test acc 0.48999998966852826\n",
            "Step 2880 Loss nan Train acc 0.3616666653090053 Test acc 0.5199999809265137\n",
            "Final Train acc 0.3686111147205035 Test acc 0.48333332935969037\n",
            "autodiff 0.7 Train acc 36.861% (0.000%)\n",
            "autodiff 0.7 Test acc 48.333% (0.000%)\n",
            "Step 72 Loss nan Train acc 0.2775000006788307 Test acc 0.5100000003973643\n",
            "Step 144 Loss nan Train acc 0.27583333518770004 Test acc 0.49333332975705463\n",
            "Step 216 Loss nan Train acc 0.2755555560191472 Test acc 0.4666666587193807\n",
            "Step 288 Loss nan Train acc 0.27194444379872745 Test acc 0.5200000007947286\n",
            "Step 360 Loss nan Train acc 0.26972222410970265 Test acc 0.5033333202203115\n",
            "Step 432 Loss nan Train acc 0.27638889145519996 Test acc 0.5066666503747305\n",
            "Step 504 Loss nan Train acc 0.2808333345585399 Test acc 0.49000000953674316\n",
            "Step 576 Loss nan Train acc 0.2783333361148834 Test acc 0.5166666507720947\n",
            "Step 648 Loss nan Train acc 0.2725000013079908 Test acc 0.5066666603088379\n",
            "Step 720 Loss nan Train acc 0.27888889031277764 Test acc 0.48333332935969037\n",
            "Step 792 Loss nan Train acc 0.27694444275564617 Test acc 0.49333332975705463\n",
            "Step 864 Loss nan Train acc 0.2755555572609107 Test acc 0.5100000103314718\n",
            "Step 936 Loss nan Train acc 0.28305555714501274 Test acc 0.5\n",
            "Step 1008 Loss nan Train acc 0.27361111467083293 Test acc 0.49666666984558105\n",
            "Step 1080 Loss nan Train acc 0.27638889186912113 Test acc 0.4966666599114736\n",
            "Step 1152 Loss nan Train acc 0.2775000006788307 Test acc 0.5166666805744171\n",
            "Step 1224 Loss nan Train acc 0.27722222233812016 Test acc 0.4866666595141093\n",
            "Step 1296 Loss nan Train acc 0.27888889031277764 Test acc 0.503333330154419\n",
            "Step 1368 Loss nan Train acc 0.27833333487312 Test acc 0.49666664997736615\n",
            "Step 1440 Loss nan Train acc 0.27388888970017433 Test acc 0.5\n",
            "Step 1512 Loss nan Train acc 0.28083333497246105 Test acc 0.5266666809717814\n",
            "Step 1584 Loss nan Train acc 0.2705555558204651 Test acc 0.4966666599114736\n",
            "Step 1656 Loss nan Train acc 0.2758333327041732 Test acc 0.4899999996026357\n",
            "Step 1728 Loss nan Train acc 0.2755555576748318 Test acc 0.4899999996026357\n",
            "Step 1800 Loss nan Train acc 0.268888886190123 Test acc 0.5133333404858907\n",
            "Step 1872 Loss nan Train acc 0.2777777798473835 Test acc 0.4899999996026357\n",
            "Step 1944 Loss nan Train acc 0.27388888721664745 Test acc 0.503333330154419\n",
            "Step 2016 Loss nan Train acc 0.27694444813662106 Test acc 0.5199999908606211\n",
            "Step 2088 Loss nan Train acc 0.2800000003642506 Test acc 0.47333333889643353\n",
            "Step 2160 Loss nan Train acc 0.274999998923805 Test acc 0.4999999900658925\n",
            "Step 2232 Loss nan Train acc 0.275277778506279 Test acc 0.4999999900658925\n",
            "Step 2304 Loss nan Train acc 0.2816666666832235 Test acc 0.4899999996026357\n",
            "Step 2376 Loss nan Train acc 0.2797222232653035 Test acc 0.48999998966852826\n",
            "Step 2448 Loss nan Train acc 0.2761111135284106 Test acc 0.5233333309491476\n",
            "Step 2520 Loss nan Train acc 0.27861111155814594 Test acc 0.4899999996026357\n",
            "Step 2592 Loss nan Train acc 0.2775000010927518 Test acc 0.5066666603088379\n",
            "Step 2664 Loss nan Train acc 0.26777777779433465 Test acc 0.5133333305517832\n",
            "Step 2736 Loss nan Train acc 0.27444444555375314 Test acc 0.48666666944821674\n",
            "Step 2808 Loss nan Train acc 0.27416666638520026 Test acc 0.4899999996026357\n",
            "Step 2880 Loss nan Train acc 0.27444444762335884 Test acc 0.4899999996026357\n",
            "Final Train acc 0.2786111111442248 Test acc 0.5333333214124044\n",
            "autodiff 0.7999999999999999 Train acc 27.861% (0.000%)\n",
            "autodiff 0.7999999999999999 Test acc 53.333% (0.000%)\n",
            "Step 72 Loss nan Train acc 0.15722222274376285 Test acc 0.5066666603088379\n",
            "Step 144 Loss nan Train acc 0.1611111116492086 Test acc 0.5099999904632568\n",
            "Step 216 Loss nan Train acc 0.15805555631717047 Test acc 0.5\n",
            "Step 288 Loss nan Train acc 0.15249999923010668 Test acc 0.4933333198229472\n",
            "Step 360 Loss nan Train acc 0.15972222263614336 Test acc 0.48333332935969037\n",
            "Step 432 Loss nan Train acc 0.16027777724795872 Test acc 0.5066666603088379\n",
            "Step 504 Loss nan Train acc 0.15805555414408445 Test acc 0.49666666984558105\n",
            "Step 576 Loss nan Train acc 0.1577777796321445 Test acc 0.5066666603088379\n",
            "Step 648 Loss nan Train acc 0.1602777794210447 Test acc 0.4866666595141093\n",
            "Step 720 Loss nan Train acc 0.16333333402872086 Test acc 0.5366666714350382\n",
            "Step 792 Loss nan Train acc 0.1588888903044992 Test acc 0.4466666678587596\n",
            "Step 864 Loss nan Train acc 0.16194444511913592 Test acc 0.5266666611035665\n",
            "Step 936 Loss nan Train acc 0.1586111115498675 Test acc 0.49666666984558105\n",
            "Step 1008 Loss nan Train acc 0.15916666719648573 Test acc 0.5\n",
            "Step 1080 Loss nan Train acc 0.16249999983443153 Test acc 0.5299999912579855\n",
            "Step 1152 Loss nan Train acc 0.157222223157684 Test acc 0.46333332856496173\n",
            "Step 1224 Loss nan Train acc 0.1561111124853293 Test acc 0.5033333400885264\n",
            "Step 1296 Loss nan Train acc 0.15833333445092043 Test acc 0.5066666603088379\n",
            "Step 1368 Loss nan Train acc 0.15583333352373707 Test acc 0.46666666865348816\n",
            "Step 1440 Loss nan Train acc 0.16222222356332672 Test acc 0.5433333218097687\n",
            "Step 1512 Loss nan Train acc 0.15722222191592058 Test acc 0.48999998966852826\n",
            "Step 1584 Loss nan Train acc 0.15666666689018408 Test acc 0.48999998966852826\n",
            "Step 1656 Loss nan Train acc 0.1594444445023934 Test acc 0.5099999904632568\n",
            "Step 1728 Loss nan Train acc 0.15694444667961863 Test acc 0.5\n",
            "Step 1800 Loss nan Train acc 0.15805555714501274 Test acc 0.503333330154419\n",
            "Step 1872 Loss nan Train acc 0.16166666833062968 Test acc 0.5166666607062022\n",
            "Step 1944 Loss nan Train acc 0.15833333362307814 Test acc 0.4666666587193807\n",
            "Step 2016 Loss nan Train acc 0.1666666670805878 Test acc 0.5166666607062022\n",
            "Step 2088 Loss nan Train acc 0.16333333568440545 Test acc 0.5099999904632568\n",
            "Step 2160 Loss nan Train acc 0.15555555580390823 Test acc 0.4866666595141093\n",
            "Step 2232 Loss nan Train acc 0.15861111165334782 Test acc 0.4833333392937978\n",
            "Step 2304 Loss nan Train acc 0.16138888895511627 Test acc 0.5199999908606211\n",
            "Step 2376 Loss nan Train acc 0.1552777789119217 Test acc 0.503333330154419\n",
            "Step 2448 Loss nan Train acc 0.15805555486844647 Test acc 0.4933333198229472\n",
            "Step 2520 Loss nan Train acc 0.15722222191592058 Test acc 0.4666666587193807\n",
            "Step 2592 Loss nan Train acc 0.1597222224291828 Test acc 0.5333333412806193\n",
            "Step 2664 Loss nan Train acc 0.15388888803621134 Test acc 0.5133333206176758\n",
            "Step 2736 Loss nan Train acc 0.16361111071374682 Test acc 0.48000000913937885\n",
            "Step 2808 Loss nan Train acc 0.15777777694165707 Test acc 0.5266666611035665\n",
            "Step 2880 Loss nan Train acc 0.16138888895511627 Test acc 0.4866666595141093\n",
            "Final Train acc 0.15888889071842036 Test acc 0.49333332975705463\n",
            "autodiff 0.8999999999999999 Train acc 15.889% (0.000%)\n",
            "autodiff 0.8999999999999999 Test acc 49.333% (0.000%)\n",
            "Step 72 Loss nan Train acc 0.4330555564827389 Test acc 0.5066666702429453\n",
            "Step 144 Loss nan Train acc 0.4372222183479203 Test acc 0.4966666599114736\n",
            "Step 216 Loss nan Train acc 0.43111111058129203 Test acc 0.4799999992052714\n",
            "Step 288 Loss nan Train acc 0.43611111243565875 Test acc 0.5400000015894572\n",
            "Step 360 Loss nan Train acc 0.44138888352447087 Test acc 0.4499999980131785\n",
            "Step 432 Loss nan Train acc 0.4280555546283722 Test acc 0.5266666611035665\n",
            "Step 504 Loss nan Train acc 0.4308333305848969 Test acc 0.4833333392937978\n",
            "Step 576 Loss nan Train acc 0.4380555533700519 Test acc 0.5200000007947286\n",
            "Step 648 Loss nan Train acc 0.4338888857099745 Test acc 0.5099999904632568\n",
            "Step 720 Loss nan Train acc 0.42944444136487114 Test acc 0.4866666595141093\n",
            "Step 792 Loss nan Train acc 0.4336111106806331 Test acc 0.5200000007947286\n",
            "Step 864 Loss nan Train acc 0.44111111015081406 Test acc 0.4799999992052714\n",
            "Step 936 Loss nan Train acc 0.4402777718173133 Test acc 0.4899999996026357\n",
            "Step 1008 Loss nan Train acc 0.42944443888134426 Test acc 0.5133333305517832\n",
            "Step 1080 Loss nan Train acc 0.4361111107799742 Test acc 0.5000000099341074\n",
            "Step 1152 Loss nan Train acc 0.43611110746860504 Test acc 0.4766666690508525\n",
            "Step 1224 Loss nan Train acc 0.43388888653781676 Test acc 0.5066666603088379\n",
            "Step 1296 Loss nan Train acc 0.43166666560702854 Test acc 0.5133333404858907\n",
            "Step 1368 Loss nan Train acc 0.43138888561063343 Test acc 0.47333333889643353\n",
            "Step 1440 Loss nan Train acc 0.4258333303862148 Test acc 0.5400000015894572\n",
            "Step 1512 Loss nan Train acc 0.43638888912068474 Test acc 0.5066666801770529\n",
            "Step 1584 Loss nan Train acc 0.4380555541978942 Test acc 0.4699999988079071\n",
            "Step 1656 Loss nan Train acc 0.4380555533700519 Test acc 0.4966666599114736\n",
            "Step 1728 Loss nan Train acc 0.4372222208314472 Test acc 0.5133333305517832\n",
            "Step 1800 Loss nan Train acc 0.42305555442969006 Test acc 0.5266666809717814\n",
            "Step 1872 Loss nan Train acc 0.4277777754598194 Test acc 0.4599999984105428\n",
            "Step 1944 Loss nan Train acc 0.4338888857099745 Test acc 0.5100000003973643\n",
            "Step 2016 Loss nan Train acc 0.43222221980492276 Test acc 0.5299999912579855\n",
            "Step 2088 Loss nan Train acc 0.42888888468345004 Test acc 0.4799999992052714\n",
            "Step 2160 Loss nan Train acc 0.43694444249073666 Test acc 0.49333332975705463\n",
            "Step 2232 Loss nan Train acc 0.44083332932657665 Test acc 0.4866666595141093\n",
            "Step 2304 Loss nan Train acc 0.4319444422920545 Test acc 0.5166666706403097\n",
            "Step 2376 Loss nan Train acc 0.4344444390800264 Test acc 0.48999998966852826\n",
            "Step 2448 Loss nan Train acc 0.43444444404708016 Test acc 0.5\n",
            "Step 2520 Loss nan Train acc 0.43888888673649895 Test acc 0.4966666599114736\n",
            "Step 2592 Loss nan Train acc 0.43027777555916047 Test acc 0.5100000003973643\n",
            "Step 2664 Loss nan Train acc 0.4388888892200258 Test acc 0.49333332975705463\n",
            "Step 2736 Loss nan Train acc 0.43194444063636994 Test acc 0.48666666944821674\n",
            "Step 2808 Loss nan Train acc 0.42944444384839797 Test acc 0.5033333400885264\n",
            "Step 2880 Loss nan Train acc 0.4427777760558658 Test acc 0.5\n",
            "Step 2952 Loss nan Train acc 0.4394444442457623 Test acc 0.5166666607062022\n",
            "Step 3024 Loss nan Train acc 0.43111110892560744 Test acc 0.4999999900658925\n",
            "Step 3096 Loss nan Train acc 0.441666663520866 Test acc 0.5033333202203115\n",
            "Step 3168 Loss nan Train acc 0.44055555512507755 Test acc 0.5066666603088379\n",
            "Step 3240 Loss nan Train acc 0.4261111070712407 Test acc 0.503333330154419\n",
            "Step 3312 Loss nan Train acc 0.4349999949336052 Test acc 0.4799999992052714\n",
            "Step 3384 Loss nan Train acc 0.4299999963906076 Test acc 0.48999998966852826\n",
            "Step 3456 Loss nan Train acc 0.4322222214606073 Test acc 0.5166666507720947\n",
            "Step 3528 Loss nan Train acc 0.43638888994852704 Test acc 0.4999999900658925\n",
            "Step 3600 Loss nan Train acc 0.431388887266318 Test acc 0.4899999996026357\n",
            "Step 3672 Loss nan Train acc 0.4374999983443154 Test acc 0.5099999904632568\n",
            "Step 3744 Loss nan Train acc 0.4313888864384757 Test acc 0.5\n",
            "Step 3816 Loss nan Train acc 0.432499997317791 Test acc 0.5099999904632568\n",
            "Step 3888 Loss nan Train acc 0.4338888857099745 Test acc 0.48999998966852826\n",
            "Step 3960 Loss nan Train acc 0.44138888683584 Test acc 0.5133333404858907\n",
            "Step 4032 Loss nan Train acc 0.44083333015441895 Test acc 0.476666659116745\n",
            "Step 4104 Loss nan Train acc 0.4352777749300003 Test acc 0.503333330154419\n",
            "Step 4176 Loss nan Train acc 0.4366666658057107 Test acc 0.5133333305517832\n",
            "Step 4248 Loss nan Train acc 0.4330555556548966 Test acc 0.503333330154419\n",
            "Step 4320 Loss nan Train acc 0.4347222215599484 Test acc 0.4899999996026357\n",
            "Step 4392 Loss nan Train acc 0.4355555557542377 Test acc 0.4966666599114736\n",
            "Step 4464 Loss nan Train acc 0.43111111058129203 Test acc 0.49666664997736615\n",
            "Step 4536 Loss nan Train acc 0.43388888736565906 Test acc 0.5\n",
            "Step 4608 Loss nan Train acc 0.43194444394773907 Test acc 0.5\n",
            "Step 4680 Loss nan Train acc 0.44166666517655057 Test acc 0.4999999900658925\n",
            "Step 4752 Loss nan Train acc 0.4372222191757626 Test acc 0.5066666503747305\n",
            "Step 4824 Loss nan Train acc 0.43944444341792 Test acc 0.4966666599114736\n",
            "Step 4896 Loss nan Train acc 0.4330555515156852 Test acc 0.5299999912579855\n",
            "Step 4968 Loss nan Train acc 0.43638888663715786 Test acc 0.45000000794728595\n",
            "Step 5040 Loss nan Train acc 0.4333333331677649 Test acc 0.5233333309491476\n",
            "Step 5112 Loss nan Train acc 0.4369444391793675 Test acc 0.5033333202203115\n",
            "Step 5184 Loss nan Train acc 0.4374999991721577 Test acc 0.4966666599114736\n",
            "Step 5256 Loss nan Train acc 0.42749999711910885 Test acc 0.4966666599114736\n",
            "Step 5328 Loss nan Train acc 0.4305555497606595 Test acc 0.523333340883255\n",
            "Step 5400 Loss nan Train acc 0.43416666405068505 Test acc 0.4599999984105428\n",
            "Step 5472 Loss nan Train acc 0.4380555558535788 Test acc 0.5233333309491476\n",
            "Step 5544 Loss nan Train acc 0.44416666362020707 Test acc 0.49333332975705463\n",
            "Step 5616 Loss nan Train acc 0.4344444415635533 Test acc 0.503333330154419\n",
            "Step 5688 Loss nan Train acc 0.4358333307835791 Test acc 0.4966666599114736\n",
            "Step 5760 Loss nan Train acc 0.42916666467984516 Test acc 0.5333333412806193\n",
            "Final Train acc 0.44277777522802353 Test acc 0.4699999988079071\n",
            "hardmining 0.6 Train acc 44.278% (0.000%)\n",
            "hardmining 0.6 Test acc 47.000% (0.000%)\n",
            "Step 72 Loss nan Train acc 0.3655555571118991 Test acc 0.5\n",
            "Step 144 Loss nan Train acc 0.370833336479134 Test acc 0.4766666690508525\n",
            "Step 216 Loss nan Train acc 0.363611113693979 Test acc 0.5433333218097687\n",
            "Step 288 Loss nan Train acc 0.3700000010430813 Test acc 0.4533333381017049\n",
            "Step 360 Loss nan Train acc 0.3655555546283722 Test acc 0.5366666515668234\n",
            "Step 432 Loss nan Train acc 0.3655555538005299 Test acc 0.4899999996026357\n",
            "Step 504 Loss nan Train acc 0.363888890379005 Test acc 0.4999999900658925\n",
            "Step 576 Loss nan Train acc 0.36472222126192516 Test acc 0.49000000953674316\n",
            "Step 648 Loss nan Train acc 0.3658333346247673 Test acc 0.523333340883255\n",
            "Step 720 Loss nan Train acc 0.3591666676931911 Test acc 0.46666666865348816\n",
            "Step 792 Loss nan Train acc 0.36472222002016175 Test acc 0.5333333412806193\n",
            "Step 864 Loss nan Train acc 0.3633333328697417 Test acc 0.4866666793823242\n",
            "Step 936 Loss nan Train acc 0.36944444560342365 Test acc 0.4999999900658925\n",
            "Step 1008 Loss nan Train acc 0.3655555571118991 Test acc 0.5233333309491476\n",
            "Step 1080 Loss nan Train acc 0.37138888736565906 Test acc 0.4699999988079071\n",
            "Step 1152 Loss nan Train acc 0.36888889057768715 Test acc 0.5033333202203115\n",
            "Step 1224 Loss nan Train acc 0.3588888885246383 Test acc 0.5200000107288361\n",
            "Step 1296 Loss nan Train acc 0.3677777763870027 Test acc 0.4833333392937978\n",
            "Step 1368 Loss nan Train acc 0.36888889057768715 Test acc 0.4999999900658925\n",
            "Step 1440 Loss nan Train acc 0.36055555525753236 Test acc 0.5100000003973643\n",
            "Step 1512 Loss nan Train acc 0.36027777940034866 Test acc 0.4866666595141093\n",
            "Step 1584 Loss nan Train acc 0.3611111119389534 Test acc 0.5300000011920929\n",
            "Step 1656 Loss nan Train acc 0.36722222218910855 Test acc 0.46666666865348816\n",
            "Step 1728 Loss nan Train acc 0.3683333314127392 Test acc 0.5066666702429453\n",
            "Step 1800 Loss nan Train acc 0.36750000052981907 Test acc 0.5\n",
            "Step 1872 Loss nan Train acc 0.3633333357671897 Test acc 0.4899999996026357\n",
            "Step 1944 Loss nan Train acc 0.36444444623258376 Test acc 0.5066666603088379\n",
            "Step 2016 Loss nan Train acc 0.3638888912068473 Test acc 0.5333333214124044\n",
            "Step 2088 Loss nan Train acc 0.3711111115084754 Test acc 0.4699999888737996\n",
            "Step 2160 Loss nan Train acc 0.36277777784400517 Test acc 0.5\n",
            "Step 2232 Loss nan Train acc 0.3666666696468989 Test acc 0.47999998927116394\n",
            "Step 2304 Loss nan Train acc 0.3638888895511627 Test acc 0.5100000003973643\n",
            "Step 2376 Loss nan Train acc 0.3652777804268731 Test acc 0.5200000007947286\n",
            "Step 2448 Loss nan Train acc 0.3661111129654778 Test acc 0.4999999900658925\n",
            "Step 2520 Loss nan Train acc 0.365833333796925 Test acc 0.5133333305517832\n",
            "Step 2592 Loss nan Train acc 0.36888888892200256 Test acc 0.4766666690508525\n",
            "Step 2664 Loss nan Train acc 0.37416666746139526 Test acc 0.49666666984558105\n",
            "Step 2736 Loss nan Train acc 0.3652777746319771 Test acc 0.5166666507720947\n",
            "Step 2808 Loss nan Train acc 0.36472222126192516 Test acc 0.49666666984558105\n",
            "Step 2880 Loss nan Train acc 0.3711111115084754 Test acc 0.4899999996026357\n",
            "Step 2952 Loss nan Train acc 0.3683333322405815 Test acc 0.4966666599114736\n",
            "Step 3024 Loss nan Train acc 0.37000000062916016 Test acc 0.5033333202203115\n",
            "Step 3096 Loss nan Train acc 0.3649999996026357 Test acc 0.5133333206176758\n",
            "Step 3168 Loss nan Train acc 0.36222222364611095 Test acc 0.503333330154419\n",
            "Step 3240 Loss nan Train acc 0.36750000052981907 Test acc 0.503333330154419\n",
            "Step 3312 Loss nan Train acc 0.37333333409494823 Test acc 0.48999998966852826\n",
            "Step 3384 Loss nan Train acc 0.3725000015563435 Test acc 0.4966666599114736\n",
            "Step 3456 Loss nan Train acc 0.3602777769168218 Test acc 0.4933333396911621\n",
            "Step 3528 Loss nan Train acc 0.3702777806255553 Test acc 0.5033333400885264\n",
            "Step 3600 Loss nan Train acc 0.3625000011589792 Test acc 0.4999999801317851\n",
            "Step 3672 Loss nan Train acc 0.3658333354526096 Test acc 0.4899999996026357\n",
            "Step 3744 Loss nan Train acc 0.3658333371082942 Test acc 0.5133333206176758\n",
            "Step 3816 Loss nan Train acc 0.36249999950329465 Test acc 0.48333332935969037\n",
            "Step 3888 Loss nan Train acc 0.37611111005147296 Test acc 0.5099999904632568\n",
            "Step 3960 Loss nan Train acc 0.36194444447755814 Test acc 0.4899999797344208\n",
            "Step 4032 Loss nan Train acc 0.3700000022848447 Test acc 0.5166666805744171\n",
            "Step 4104 Loss nan Train acc 0.3566666700773769 Test acc 0.5166666706403097\n",
            "Step 4176 Loss nan Train acc 0.3680555563833978 Test acc 0.503333330154419\n",
            "Step 4248 Loss nan Train acc 0.3677777788705296 Test acc 0.4866666595141093\n",
            "Step 4320 Loss nan Train acc 0.36888888974984485 Test acc 0.4866666595141093\n"
          ]
        }
      ],
      "source": [
        "experiments = {'autodiff':[], 'hardmining':[], 'ratio':[], 'random':[], 'baseline':[]}\n",
        "for exp_name in ['autodiff', 'hardmining', 'ratio', 'random', 'baseline']:\n",
        "  experiments[exp_name] = run_many_imbalance(data_img, exp_name, nrun=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LfSO75R3aN1",
        "outputId": "6e828b32-2735-4c61-f791-0629fcc9bb0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "random\n",
            "Step 72 Loss 0.29923537 Train acc 0.9213888926638497 Test acc 0.0\n",
            "Step 144 Loss 0.20141357 Train acc 0.9211111184623506 Test acc 0.0\n",
            "Step 216 Loss 0.3088569 Train acc 0.9202777793010076 Test acc 0.0\n",
            "Step 288 Loss 0.26012236 Train acc 0.9202777793010076 Test acc 0.0\n",
            "Step 360 Loss 0.47783712 Train acc 0.9252777778440051 Test acc 0.0\n",
            "Step 432 Loss 0.18504608 Train acc 0.9200000017881393 Test acc 0.0\n",
            "Step 504 Loss 0.3292215 Train acc 0.9249999953640832 Test acc 0.0\n",
            "Step 576 Loss 0.15184894 Train acc 0.9188888900809817 Test acc 0.0\n",
            "Step 648 Loss 0.15826714 Train acc 0.9194444467624029 Test acc 0.0\n",
            "Step 720 Loss 0.20386183 Train acc 0.9213888893524805 Test acc 0.0\n",
            "Step 792 Loss 0.20112236 Train acc 0.9188888917366663 Test acc 0.0\n",
            "Step 864 Loss 0.28428543 Train acc 0.9233333385652966 Test acc 0.0\n",
            "Step 936 Loss 0.29409814 Train acc 0.91916666759385 Test acc 0.0\n",
            "Step 1008 Loss 0.2119308 Train acc 0.9202777809566922 Test acc 0.0\n",
            "Step 1080 Loss 0.3704489 Train acc 0.9213888893524805 Test acc 0.0\n",
            "Step 1152 Loss 0.1976569 Train acc 0.9216666685210334 Test acc 0.0\n",
            "Step 1224 Loss 0.37321368 Train acc 0.91916666759385 Test acc 0.0\n",
            "Step 1296 Loss 0.2750952 Train acc 0.9200000017881393 Test acc 0.0\n",
            "Step 1368 Loss 0.52712405 Train acc 0.921111113495297 Test acc 0.0\n",
            "Step 1440 Loss 0.15704957 Train acc 0.9219444443782171 Test acc 0.0\n",
            "Step 1512 Loss 0.24436441 Train acc 0.9238888902796639 Test acc 0.0\n",
            "Step 1584 Loss 0.3920176 Train acc 0.9188888933923509 Test acc 0.0\n",
            "Step 1656 Loss 0.11479991 Train acc 0.9197222275866402 Test acc 0.0\n",
            "Step 1728 Loss 0.18959716 Train acc 0.9227777835395601 Test acc 0.0\n",
            "Step 1800 Loss 0.23708893 Train acc 0.9230555577410592 Test acc 0.0\n",
            "Step 1872 Loss 0.23386681 Train acc 0.9200000050995085 Test acc 0.0\n",
            "Step 1944 Loss 0.1996502 Train acc 0.9194444484180875 Test acc 0.0\n",
            "Step 2016 Loss 0.37906897 Train acc 0.9186111158794827 Test acc 0.0\n",
            "Step 2088 Loss 0.08909951 Train acc 0.9211111151509814 Test acc 0.0\n",
            "Step 2160 Loss 0.18217614 Train acc 0.920277777645323 Test acc 0.0\n",
            "Step 2232 Loss 0.3774762 Train acc 0.9177777767181396 Test acc 0.0\n",
            "Step 2304 Loss 0.37355766 Train acc 0.9236111177338494 Test acc 0.0\n",
            "Step 2376 Loss 0.19079746 Train acc 0.920555560125245 Test acc 0.0\n",
            "Step 2448 Loss 0.61927676 Train acc 0.9230555560853746 Test acc 0.0\n",
            "Step 2520 Loss 0.2471933 Train acc 0.9200000050995085 Test acc 0.0\n",
            "Step 2592 Loss 0.17267008 Train acc 0.918055557542377 Test acc 0.0\n",
            "Step 2664 Loss 0.3362397 Train acc 0.9225000010596381 Test acc 0.0\n",
            "Step 2736 Loss 0.15785314 Train acc 0.9194444467624029 Test acc 0.0\n",
            "Step 2808 Loss 0.18395816 Train acc 0.9202777743339539 Test acc 0.0\n",
            "Step 2880 Loss 0.07357333 Train acc 0.920555560125245 Test acc 0.0\n",
            "Step 2952 Loss 0.12331756 Train acc 0.9241666711039014 Test acc 0.0\n",
            "Step 3024 Loss 0.26111713 Train acc 0.919999995165401 Test acc 0.0\n",
            "Step 3096 Loss 0.37090847 Train acc 0.9199999984767702 Test acc 0.0\n",
            "Step 3168 Loss 0.13824914 Train acc 0.9219444427225325 Test acc 0.0\n",
            "Step 3240 Loss 0.15522492 Train acc 0.9219444493452708 Test acc 0.0\n",
            "Step 3312 Loss 0.12897047 Train acc 0.9208333343267441 Test acc 0.0\n",
            "Step 3384 Loss 0.34209466 Train acc 0.9180555625094308 Test acc 0.0\n",
            "Step 3456 Loss 0.23778148 Train acc 0.9183333317438761 Test acc 0.0\n",
            "Step 3528 Loss 0.3230819 Train acc 0.9183333300881915 Test acc 0.0\n",
            "Step 3600 Loss 0.30936202 Train acc 0.9230555593967438 Test acc 0.0\n",
            "Step 3672 Loss 0.35768092 Train acc 0.9230555577410592 Test acc 0.0\n",
            "Step 3744 Loss 0.15819451 Train acc 0.9188888967037201 Test acc 0.0\n",
            "Step 3816 Loss 0.12695818 Train acc 0.9219444460339017 Test acc 0.0\n",
            "Step 3888 Loss 0.43351045 Train acc 0.9177777800295088 Test acc 0.0\n",
            "Step 3960 Loss 0.5018577 Train acc 0.9188888900809817 Test acc 0.0\n",
            "Step 4032 Loss 0.19414416 Train acc 0.9188888851139281 Test acc 0.0\n",
            "Step 4104 Loss 0.25739112 Train acc 0.9238888952467177 Test acc 0.0\n",
            "Step 4176 Loss 0.10031984 Train acc 0.9213888860411115 Test acc 0.0\n",
            "Step 4248 Loss 0.20166521 Train acc 0.921111113495297 Test acc 0.0\n",
            "Step 4320 Loss 0.16965114 Train acc 0.9222222218910853 Test acc 0.0\n",
            "Step 4392 Loss 0.16099782 Train acc 0.9244444486167696 Test acc 0.0\n",
            "Step 4464 Loss 0.67184854 Train acc 0.920277777645323 Test acc 0.0\n",
            "Step 4536 Loss 0.36519796 Train acc 0.9169444458352195 Test acc 0.0\n",
            "Step 4608 Loss 0.128938 Train acc 0.9205555568138758 Test acc 0.0\n",
            "Step 4680 Loss 0.49592352 Train acc 0.9197222259309557 Test acc 0.0\n",
            "Step 4752 Loss 0.24832767 Train acc 0.9197222259309557 Test acc 0.0\n",
            "Step 4824 Loss 0.2515515 Train acc 0.9208333376381133 Test acc 0.0\n",
            "Step 4896 Loss 0.13447714 Train acc 0.924166664481163 Test acc 0.0\n",
            "Step 4968 Loss 0.18358105 Train acc 0.9219444460339017 Test acc 0.0\n",
            "Step 5040 Loss 0.2989975 Train acc 0.9188888933923509 Test acc 0.0\n",
            "Step 5112 Loss 0.18020144 Train acc 0.9205555634366142 Test acc 0.0\n",
            "Step 5184 Loss 0.22841428 Train acc 0.9247222277853224 Test acc 0.0\n",
            "Step 5256 Loss 0.28310317 Train acc 0.9250000036425061 Test acc 0.0\n",
            "Step 5328 Loss 0.29639703 Train acc 0.9177777700954013 Test acc 0.0\n",
            "Step 5400 Loss 0.40602332 Train acc 0.920555560125245 Test acc 0.0\n",
            "Step 5472 Loss 0.31855664 Train acc 0.9155555582708783 Test acc 0.0\n",
            "Step 5544 Loss 0.39348227 Train acc 0.9216666685210334 Test acc 0.0\n",
            "Step 5616 Loss 0.2892022 Train acc 0.9186111142237982 Test acc 0.0\n",
            "Step 5688 Loss 0.3688167 Train acc 0.920555560125245 Test acc 0.0\n",
            "Step 5760 Loss 0.35218978 Train acc 0.9219444443782171 Test acc 0.0\n",
            "Final Train acc 0.9205555551581912 Test acc 0.0\n",
            "random 0.95 Train acc 92.056% (0.000%)\n",
            "random 0.95 Test acc 0.000% (0.000%)\n",
            "baseline\n",
            "Step 72 Loss 0.25448585 Train acc 0.9200000034438239 Test acc 0.0\n",
            "Step 144 Loss 0.33739096 Train acc 0.9194444467624029 Test acc 0.0\n",
            "Step 216 Loss 0.2567087 Train acc 0.9188888950480355 Test acc 0.0\n",
            "Step 288 Loss 0.25365543 Train acc 0.918055557542377 Test acc 0.0\n",
            "Step 360 Loss 0.30250362 Train acc 0.9208333343267441 Test acc 0.0\n",
            "Step 432 Loss 0.30600443 Train acc 0.9211111101839278 Test acc 0.0\n",
            "Step 504 Loss 0.2023168 Train acc 0.9233333369096121 Test acc 0.0\n",
            "Step 576 Loss 0.19914925 Train acc 0.9208333326710595 Test acc 0.0\n",
            "Step 648 Loss 0.17817508 Train acc 0.9174999992052714 Test acc 0.0\n",
            "Step 720 Loss 0.34931663 Train acc 0.9247222228182687 Test acc 0.0\n",
            "Step 792 Loss 0.32539997 Train acc 0.922222226858139 Test acc 0.0\n",
            "Step 864 Loss 0.23377885 Train acc 0.9208333359824287 Test acc 0.0\n",
            "Step 936 Loss 0.27903014 Train acc 0.9216666685210334 Test acc 0.0\n",
            "Step 1008 Loss 0.35945916 Train acc 0.9225000010596381 Test acc 0.0\n",
            "Step 1080 Loss 0.32581937 Train acc 0.919722220963902 Test acc 0.0\n",
            "Step 1152 Loss 0.22777718 Train acc 0.922222226858139 Test acc 0.0\n",
            "Step 1224 Loss 0.5092394 Train acc 0.918611110912429 Test acc 0.0\n",
            "Step 1296 Loss 0.27960405 Train acc 0.9258333361811109 Test acc 0.0\n",
            "Step 1368 Loss 0.22681217 Train acc 0.9222222202354007 Test acc 0.0\n",
            "Step 1440 Loss 0.27860293 Train acc 0.921666670176718 Test acc 0.0\n",
            "Step 1512 Loss 0.46288383 Train acc 0.9208333409494824 Test acc 0.0\n",
            "Step 1584 Loss 0.19847088 Train acc 0.9161111149522994 Test acc 0.0\n",
            "Step 1656 Loss 0.25451168 Train acc 0.9202777809566922 Test acc 0.0\n",
            "Step 1728 Loss 0.28135657 Train acc 0.9230555610524284 Test acc 0.0\n",
            "Step 1800 Loss 0.27894264 Train acc 0.9177777849965625 Test acc 0.0\n",
            "Step 1872 Loss 0.20141889 Train acc 0.9227777785725064 Test acc 0.0\n",
            "Step 1944 Loss 0.33090383 Train acc 0.9255555586682426 Test acc 0.0\n",
            "Step 2016 Loss 0.38102183 Train acc 0.9188888917366663 Test acc 0.0\n",
            "Step 2088 Loss 0.2795472 Train acc 0.9200000050995085 Test acc 0.0\n",
            "Step 2160 Loss 0.20621847 Train acc 0.9205555617809296 Test acc 0.0\n",
            "Step 2232 Loss 0.3381466 Train acc 0.918055557542377 Test acc 0.0\n",
            "Step 2304 Loss 0.313817 Train acc 0.921111113495297 Test acc 0.0\n",
            "Step 2376 Loss 0.22770949 Train acc 0.918055557542377 Test acc 0.0\n",
            "Step 2448 Loss 0.33001485 Train acc 0.922222226858139 Test acc 0.0\n",
            "Step 2520 Loss 0.179746 Train acc 0.9177777734067705 Test acc 0.0\n",
            "Step 2592 Loss 0.3298847 Train acc 0.9202777793010076 Test acc 0.0\n",
            "Step 2664 Loss 0.30664387 Train acc 0.9211111151509814 Test acc 0.0\n",
            "Step 2736 Loss 0.20982456 Train acc 0.9205555551581912 Test acc 0.0\n",
            "Step 2808 Loss 0.25338766 Train acc 0.9202777826123767 Test acc 0.0\n",
            "Step 2880 Loss 0.25407004 Train acc 0.9227777818838755 Test acc 0.0\n",
            "Step 2952 Loss 0.23151714 Train acc 0.9191666659381654 Test acc 0.0\n",
            "Step 3024 Loss 0.27889827 Train acc 0.9194444434510337 Test acc 0.0\n",
            "Step 3096 Loss 0.20497543 Train acc 0.9200000034438239 Test acc 0.0\n",
            "Step 3168 Loss 0.33784255 Train acc 0.9186111125681136 Test acc 0.0\n",
            "Step 3240 Loss 0.3293372 Train acc 0.9183333350552453 Test acc 0.0\n",
            "Step 3312 Loss 0.3025405 Train acc 0.9236111160781648 Test acc 0.0\n",
            "Step 3384 Loss 0.27898175 Train acc 0.9208333343267441 Test acc 0.0\n",
            "Step 3456 Loss 0.3581675 Train acc 0.919722220963902 Test acc 0.0\n",
            "Step 3528 Loss 0.4405575 Train acc 0.9208333359824287 Test acc 0.0\n",
            "Step 3600 Loss 0.27905646 Train acc 0.921666670176718 Test acc 0.0\n",
            "Step 3672 Loss 0.2590608 Train acc 0.9186111092567444 Test acc 0.0\n",
            "Step 3744 Loss 0.2540929 Train acc 0.9227777835395601 Test acc 0.0\n",
            "Step 3816 Loss 0.30095208 Train acc 0.9172222233480878 Test acc 0.0\n",
            "Step 3888 Loss 0.36301532 Train acc 0.9225000076823764 Test acc 0.0\n",
            "Step 3960 Loss 0.27795565 Train acc 0.9172222250037723 Test acc 0.0\n",
            "Step 4032 Loss 0.35820526 Train acc 0.9194444467624029 Test acc 0.0\n",
            "Step 4104 Loss 0.36415815 Train acc 0.9180555608537462 Test acc 0.0\n",
            "Step 4176 Loss 0.4402429 Train acc 0.9202777809566922 Test acc 0.0\n",
            "Step 4248 Loss 0.28183183 Train acc 0.9236111111111112 Test acc 0.0\n",
            "Step 4320 Loss 0.3606275 Train acc 0.91916666759385 Test acc 0.0\n",
            "Step 4392 Loss 0.3274839 Train acc 0.9186111175351672 Test acc 0.0\n",
            "Step 4464 Loss 0.22694883 Train acc 0.9213888910081651 Test acc 0.0\n",
            "Step 4536 Loss 0.25475675 Train acc 0.9208333392937978 Test acc 0.0\n",
            "Step 4608 Loss 0.3757509 Train acc 0.9213888910081651 Test acc 0.0\n",
            "Step 4680 Loss 0.15034012 Train acc 0.9202777809566922 Test acc 0.0\n",
            "Step 4752 Loss 0.20506592 Train acc 0.9208333343267441 Test acc 0.0\n",
            "Step 4824 Loss 0.23992558 Train acc 0.9208333343267441 Test acc 0.0\n",
            "Step 4896 Loss 0.22974083 Train acc 0.916388890809483 Test acc 0.0\n",
            "Step 4968 Loss 0.22624303 Train acc 0.9197222242752711 Test acc 0.0\n",
            "Step 5040 Loss 0.3783944 Train acc 0.9222222252024544 Test acc 0.0\n",
            "Step 5112 Loss 0.25569835 Train acc 0.9225000010596381 Test acc 0.0\n",
            "Step 5184 Loss 0.30113342 Train acc 0.921111116806666 Test acc 0.0\n",
            "Step 5256 Loss 0.30319774 Train acc 0.9222222252024544 Test acc 0.0\n",
            "Step 5328 Loss 0.30109426 Train acc 0.9202777759896384 Test acc 0.0\n",
            "Step 5400 Loss 0.35143718 Train acc 0.921111113495297 Test acc 0.0\n",
            "Step 5472 Loss 0.30296245 Train acc 0.9205555584695604 Test acc 0.0\n",
            "Step 5544 Loss 0.30355734 Train acc 0.9205555568138758 Test acc 0.0\n",
            "Step 5616 Loss 0.30270192 Train acc 0.9213888810740577 Test acc 0.0\n",
            "Step 5688 Loss 0.20272833 Train acc 0.9236111127667956 Test acc 0.0\n",
            "Step 5760 Loss 0.17370078 Train acc 0.917500000860956 Test acc 0.0\n",
            "Final Train acc 0.9227777802281909 Test acc 0.0\n",
            "baseline 0.95 Train acc 92.278% (0.000%)\n",
            "baseline 0.95 Test acc 0.000% (0.000%)\n"
          ]
        }
      ],
      "source": [
        "for exp_name in ['random', 'baseline']:\n",
        "  run_many_imbalance(data_img, exp_name, nrun=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0oMUxK_js2a"
      },
      "outputs": [],
      "source": [
        "\n",
        "def reweight_random(bsize, eps=0.0):\n",
        "    \"\"\"Reweight examples using random numbers.\n",
        "    \n",
        "    :param bsize:             [int]       Batch size.\n",
        "    :param eps:               [float]     Minimum example weights, default 0.0.\n",
        "    \"\"\"\n",
        "    ex_weight = tf.compat.v1.random_normal([bsize], mean=0.0, stddev=1.0)\n",
        "    ex_weight_plus = tf.maximum(ex_weight, eps)\n",
        "    ex_weight_sum = tf.reduce_sum(ex_weight_plus)\n",
        "    ex_weight_sum += tf.cast(tf.equal(ex_weight_sum, 0.0), tf.float32)\n",
        "    ex_weight_norm = ex_weight_plus / ex_weight_sum\n",
        "    return ex_weight_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNBHebGqjvYk"
      },
      "outputs": [],
      "source": [
        "def reweight_hard_mining(inp, label, positive=False):\n",
        "    \"\"\"Reweight examples using hard mining.\n",
        "    \n",
        "    :param inp:          [Tensor]   [N, ...] Inputs.\n",
        "    :param label:        [Tensor]   [N] Labels\n",
        "    :param positive:     [bool]     Whether perform hard positive mining or hard negative mining.\n",
        "    :return              [Tensor]   Examples weights of the same shape as the first dim of inp.\n",
        "    \"\"\"\n",
        "    _, loss, logits = get_model(inp, label, ex_wts=None, is_training=True, reuse=True)\n",
        "\n",
        "    # Mine for positive\n",
        "    if positive:\n",
        "        loss_mask = loss * label\n",
        "    else:\n",
        "        loss_mask = loss * (1 - label)\n",
        "\n",
        "    if positive:\n",
        "        k = tf.cast(tf.reduce_sum(1 - label), tf.int32)\n",
        "    else:\n",
        "        k = tf.cast(tf.reduce_sum(label), tf.int32)\n",
        "    k = tf.maximum(k, 1)\n",
        "    loss_sorted, loss_sort_idx = tf.nn.top_k(loss_mask, k)\n",
        "\n",
        "    if positive:\n",
        "        mask = 1 - label\n",
        "    else:\n",
        "        mask = label\n",
        "\n",
        "    updates = tf.ones([tf.shape(loss_sort_idx)[0]], dtype=label.dtype)\n",
        "    mask_add = tf.scatter_nd(tf.expand_dims(loss_sort_idx, axis=1), updates, [tf.shape(inp)[0]])\n",
        "    mask = tf.maximum(mask, mask_add)\n",
        "    mask_sum = tf.reduce_sum(mask)\n",
        "    mask_sum += tf.cast(tf.equal(mask_sum, 0.0), tf.float32)\n",
        "    mask = mask / mask_sum\n",
        "    return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYBB3UIvjrVd"
      },
      "outputs": [],
      "source": [
        "def reweight_autodiff(inp_a,\n",
        "                      label_a,\n",
        "                      inp_b,\n",
        "                      label_b,\n",
        "                      bsize_a,\n",
        "                      bsize_b,\n",
        "                      eps=0.0,\n",
        "                      gate_gradients=1):\n",
        "    \"\"\"Reweight examples using automatic differentiation.\n",
        "    :param inp_a:             [Tensor]    Inputs for the noisy pass.\n",
        "    :param label_a:           [Tensor]    Labels for the noisy pass.\n",
        "    :param inp_b:             [Tensor]    Inputs for the clean pass.\n",
        "    :param label_b:           [Tensor]    Labels for the clean pass.\n",
        "    :param bsize_a:           [int]       Batch size for the noisy pass.\n",
        "    :param bsize_b:           [int]       Batch size for the clean pass.\n",
        "    :param eps:               [float]     Minimum example weights, default 0.0.\n",
        "    :param gate_gradients:    [int]       Tensorflow gate gradients, reduce concurrency.\n",
        "    \"\"\"\n",
        "    ex_wts_a = tf.zeros([bsize_a], dtype=tf.float32)\n",
        "    ex_wts_b = tf.ones([bsize_b], dtype=tf.float32) / float(bsize_b)\n",
        "    w_dict, loss_a, logits_a = get_model(\n",
        "        inp_a, label_a, ex_wts=ex_wts_a, is_training=True, reuse=True)\n",
        "    var_names = w_dict.keys()\n",
        "    var_list = [w_dict[kk] for kk in var_names]\n",
        "    grads = tf.gradients(loss_a, var_list, gate_gradients=gate_gradients)\n",
        "\n",
        "    var_list_new = [vv - gg for gg, vv in zip(grads, var_list)]\n",
        "    w_dict_new = dict(zip(var_names, var_list_new))\n",
        "    _, loss_b, logits_b = get_model(\n",
        "        inp_b, label_b, ex_wts=ex_wts_b, is_training=True, reuse=True, w_dict=w_dict_new)\n",
        "    grads_ex_wts = tf.gradients(loss_b, [ex_wts_a], gate_gradients=gate_gradients)[0]\n",
        "    ex_weight = -grads_ex_wts\n",
        "    ex_weight_plus = tf.maximum(ex_weight, eps)\n",
        "    ex_weight_sum = tf.reduce_sum(ex_weight_plus)\n",
        "    ex_weight_sum += tf.cast(tf.equal(ex_weight_sum, 0.0), tf.float32)\n",
        "    ex_weight_norm = ex_weight_plus / ex_weight_sum\n",
        "    return ex_weight_norm\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Reweight_face_dataset_imbalance.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}